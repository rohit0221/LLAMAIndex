{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Replacement + Node Sentence Window\n",
    "In this notebook, we use the ```SentenceWindowNodeParser``` to parse documents into single sentences per node. Each node also contains a \"window\" with the sentences on either side of the node sentence.\n",
    "\n",
    "Then, after retrieval, before passing the retrieved sentences to the LLM, the single sentences are replaced with a window containing the surrounding sentences using the ```MetadataReplacementNodePostProcessor```.\n",
    "\n",
    "This is most useful for large documents/indexes, as it helps to retrieve more fine-grained details.\n",
    "\n",
    "By default, the sentence window is 5 sentences on either side of the original sentence.\n",
    "\n",
    "In this case, chunk size settings are not used, in favor of following the window settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-openai in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (0.2.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-embeddings-openai) (0.11.20)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-embeddings-openai) (1.52.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.23.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (24.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.2.0)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.26.2)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.11.20)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-embeddings-huggingface) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.10)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.46.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
      "Requirement already satisfied: click in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.23.0)\n",
      "Requirement already satisfied: anyio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n",
      "Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: llama-index-embeddings-huggingface\n",
      "Successfully installed llama-index-embeddings-huggingface-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-openai in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-llms-openai) (0.11.20)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-llms-openai) (1.52.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.23.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "# base node parser is a sentence splitter\n",
    "text_splitter = SentenceSplitter()\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "# embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\", max_length=512\n",
    "# )\n",
    "\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "# Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Build the Index\n",
    "In this section, we load data and build the vector index.\n",
    "\n",
    "# Load Data\n",
    "Here, we build an index using chapter 3 of the recent IPCC climate report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 47 20.7M   47  9.9M    0     0  9737k      0  0:00:02  0:00:01  0:00:01 9769k\n",
      " 87 20.7M   87 18.0M    0     0  9053k      0  0:00:02  0:00:02 --:--:-- 9067k\n",
      "100 20.7M  100 20.7M    0     0  8509k      0  0:00:02  0:00:02 --:--:-- 8520k\n"
     ]
    }
   ],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Nodes\n",
    "We extract out the set of nodes that will be stored in the VectorIndex. This includes both the nodes with the sentence window parser, as well as the \"base\" nodes extracted using the standard parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Indexes\n",
    "We build both the sentence index, as well as the \"base\" index (with default chunk sizes).\n",
    "\n",
    "📌Load the embeddings from local director if it's build already to save cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(base_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist the Embeddings:\n",
    "After generating the embeddings, call the persist method on your vector store instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(persist_dir=\"./my_text_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index.storage_context.persist(persist_dir=\"./my_base_index_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Embeddings from Local Storage\n",
    "When you want to retrieve the stored embeddings, you can load them back into your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./my_text_embeddings\")\n",
    "\n",
    "# Load the index\n",
    "sentence_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./my_base_index_embeddings\")\n",
    "\n",
    "# Load the index\n",
    "base_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying\n",
    "### With MetadataReplacementPostProcessor\n",
    "Here, we now use the ```MetadataReplacementPostProcessor``` to replace the sentence in each node with it's surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concerns surrounding the Atlantic meridional overturning circulation (AMOC) include low confidence in quantifying its changes during the 20th century due to a lack of agreement in reconstructed and simulated trends. Additionally, direct observational records since the mid-2000s are considered too short to accurately assess the contributions of internal variability, natural forcing, and anthropogenic forcing to changes in AMOC. Projections indicate that AMOC is very likely to decline over the 21st century across all Shared Socioeconomic Pathways (SSP) scenarios, although an abrupt collapse is not expected before 2100.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "window_response = query_engine.query(\n",
    "    \"What are the concerns surrounding the AMOC?\"\n",
    ")\n",
    "print(window_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also check the original sentence that was retrieved for each node, as well as the actual window of sentences that was sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window: WGI AR6 assessed that only the California Current system has \n",
      "undergone large-scale upwelling-favourable wind intensification since \n",
      "the 1980s (medium confidence) (WGI AR6 Section  9.2.1.5; García-\n",
      "Reyes and Largier, 2010; Seo et al., 2012; Fox-Kemper et al., 2021).\n",
      " While no consistent pattern of contemporary changes in upwelling-\n",
      "favourable winds emerges from observation-based studies, numerical \n",
      "and theoretical work projects that summertime winds near poleward \n",
      "boundaries of upwelling zones will intensify, while winds near \n",
      "equatorward boundaries will weaken (high confidence) (WGI AR6 \n",
      "Section  9.2.3.5; García-Reyes et  al., 2015; Rykaczewski et  al., 2015; \n",
      "Wang et  al., 2015; Aguirre et  al., 2019; Fox-Kemper et  al., 2021).  Nevertheless, projected future annual cumulative upwelling wind \n",
      "changes at most locations and seasons remain within ±10–20% of \n",
      "present-day values (medium confidence) (WGI AR6 Section  9.2.3.5; \n",
      "Fox-Kemper et al., 2021).\n",
      " Continuous observation of the Atlantic meridional overturning \n",
      "circulation (AMOC) has improved the understanding of its variability \n",
      "(Frajka-Williams et  al., 2019), but there is low confidence in the \n",
      "quantification of AMOC changes in the 20th century because of low \n",
      "agreement in quantitative reconstructed and simulated trends (WGI \n",
      "AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., 2021). \n",
      " Direct observational records since the mid-2000s remain too short to \n",
      "determine the relative contributions of internal variability, natural \n",
      "forcing and anthropogenic forcing to AMOC change (high confidence) \n",
      "(WGI AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., \n",
      "2021).  Over the 21st century, AMOC will very likely decline for all SSP \n",
      "scenarios but will not involve an abrupt collapse before 2100 (WGI \n",
      "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n",
      " 3.2.2.4 Sea Ice Changes\n",
      "Sea ice is a key driver of polar marine life, hosting unique ecosystems \n",
      "and affecting diverse marine organisms and food webs through its \n",
      "impact on light penetration and supplies of nutrients and organic \n",
      "matter (Arrigo, 2014). \n",
      "------------------\n",
      "Original Sentence: Continuous observation of the Atlantic meridional overturning \n",
      "circulation (AMOC) has improved the understanding of its variability \n",
      "(Frajka-Williams et  al., 2019), but there is low confidence in the \n",
      "quantification of AMOC changes in the 20th century because of low \n",
      "agreement in quantitative reconstructed and simulated trends (WGI \n",
      "AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., 2021). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = window_response.source_nodes[0].node.metadata[\"window\"]\n",
    "sentence = window_response.source_nodes[0].node.metadata[\"original_text\"]\n",
    "\n",
    "print(f\"Window: {window}\")\n",
    "print(\"------------------\")\n",
    "print(f\"Original Sentence: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast with normal VectorStoreIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concerns surrounding the Atlantic Meridional Overturning Circulation (AMOC) include low confidence in reconstructed and modeled changes for the 20th century, which raises uncertainties about its historical behavior. Projections indicate that the AMOC is expected to decline over the 21st century, with high confidence in this trend, although there is low confidence regarding quantitative projections of the extent of this decline. This uncertainty poses challenges for understanding the potential impacts on climate and oceanic systems.\n"
     ]
    }
   ],
   "source": [
    "query_engine = base_index.as_query_engine(similarity_top_k=2)\n",
    "vector_response = query_engine.query(\n",
    "    \"What are the concerns surrounding the AMOC?\"\n",
    ")\n",
    "print(vector_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well, that didn't work. Let's bump up the top k! This will be slower and use more tokens compared to the sentence window index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is low confidence in reconstructed and modeled changes of the Atlantic Meridional Overturning Circulation (AMOC) for the 20th century. Projections indicate that the AMOC is expected to decline over the 21st century, although there is high confidence in this trend, the confidence in quantitative projections remains low. This uncertainty raises concerns about the potential impacts on climate patterns, ocean circulation, and associated ecosystems.\n"
     ]
    }
   ],
   "source": [
    "query_engine = base_index.as_query_engine(similarity_top_k=5)\n",
    "vector_response = query_engine.query(\n",
    "    \"What are the concerns surrounding the AMOC?\"\n",
    ")\n",
    "print(vector_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "So the ```SentenceWindowNodeParser``` + ```MetadataReplacementNodePostProcessor``` combo is the clear winner here. But why?\n",
    "\n",
    "Embeddings at a sentence level seem to capture more fine-grained details, like the word ```AMOC```.\n",
    "\n",
    "We can also compare the retrieved chunks for each index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous observation of the Atlantic meridional overturning \n",
      "circulation (AMOC) has improved the understanding of its variability \n",
      "(Frajka-Williams et  al., 2019), but there is low confidence in the \n",
      "quantification of AMOC changes in the 20th century because of low \n",
      "agreement in quantitative reconstructed and simulated trends (WGI \n",
      "AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., 2021). \n",
      "\n",
      "--------\n",
      "Over the 21st century, AMOC will very likely decline for all SSP \n",
      "scenarios but will not involve an abrupt collapse before 2100 (WGI \n",
      "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n",
      "\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for source_node in window_response.source_nodes:\n",
    "    print(source_node.node.metadata[\"original_text\"])\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the sentence window index easily retrieved two nodes that talk about AMOC. Remember, the embeddings are based purely on the original sentence here, but the LLM actually ends up reading the surrounding context as well!\n",
    "\n",
    "Now, let's try and disect why the naive vector index failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMOC mentioned? False\n",
      "--------\n",
      "AMOC mentioned? True\n",
      "--------\n",
      "AMOC mentioned? False\n",
      "--------\n",
      "AMOC mentioned? False\n",
      "--------\n",
      "AMOC mentioned? False\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for node in vector_response.source_nodes:\n",
    "    print(\"AMOC mentioned?\", \"AMOC\" in node.node.text)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So source node at index [2] mentions AMOC, but what did this text actually look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "483Oceans and Coastal Ecosystems and Their Services  Chapter 3\n",
      "Figure  3.26; Bates et  al., 2014; Lubchenco and Grorud-Colvert, 2015; \n",
      "Gattuso et  al., 2018) because they alleviate non-climate drivers and \n",
      "promote biodiversity (i.e., ‘managed resilience hypothesis’) (Bruno \n",
      "et  al., 2019; Maestro et  al., 2019; Cinner et  al., 2020). Current MPAs \n",
      "offer conservation benefits such as increases in biomass and diversity \n",
      "of habitats, populations and communities (high confidence) (Pendleton \n",
      "et al., 2018; Bates et al., 2019; Stevenson et al., 2020; Lenihan et al., 2021; \n",
      "Ohayon et al., 2021), and these benefits may last after some (possibly \n",
      "climate-enhanced) disturbances (e.g., tropical cyclones) (McClure et al., \n",
      "2020). But current MPAs do not provide resilience against observed \n",
      "warming and heatwaves in tropical-to-temperate ecosystems (medium \n",
      "confidence) (Bates et al., 2019; Bruno et al., 2019; Freedman et al., 2020; \n",
      "Graham et al., 2020; Rilov et al., 2020). There is robust evidence that \n",
      "processes around MPA design and implementation strongly influence \n",
      "whether outcomes are beneficial or harmful for adjacent human \n",
      "communities (McNeill et al., 2018; Zupan et al., 2018; Ban et al., 2019).\n",
      "Current placement and extent of MPAs will not provide substantial \n",
      "protections against projected climate change past 2050 (high \n",
      "confidence), as the placement of MPAs has been driven more often \n",
      "by political expediency (e.g., Leenhardt et al., 2013) than by managing \n",
      "key drivers of biodiversity loss (Cockerell et al., 2020; Stevenson et al., \n",
      "2020) or climate-induced drivers (Bruno et  al., 2018). Only 3.5% of \n",
      "the area currently protected will provide refuges from both SST \n",
      "and deoxygenation by 2050 under both RCP4.5 and RCP8.5 (Bruno \n",
      "et al., 2018), and MPAs are more exposed to climate change under \n",
      "RCP8.5 than non-MPAs (Section 3.4.3.3.4; Figure 3.20d). Community \n",
      "thermal tolerances will be exceeded by 2050 in the tropics and by \n",
      "2150 for many higher-latitude MPAs (Bruno et al., 2018). Most MPA \n",
      "design has focused on the surface ocean, but MPAs are assumed to \n",
      "protect the entire water column and benthos. Climate-induced drivers \n",
      "(Section  3.2) throughout the water column and rapidly accelerating \n",
      "climate velocities at depths below 200 m (Johnson et al., 2018; Brito-\n",
      "Morales et al., 2020) are projected to affect virtually all North Atlantic \n",
      "deep-water and open-ocean area-based management zones in the \n",
      "next 20–50 years (Johnson et al., 2018), and the conservation goals of \n",
      "benthic MPAs in the North Sea are not expected to be fulfilled (Weinert \n",
      "et al., 2021). Heightened risk of non-indigenous species immigration \n",
      "from vessel traffic plus climate change further endangers MPA success \n",
      "(Iacarella et  al., 2020), a particular concern in the Mediterranean \n",
      "(D’Amen and Azzurro, 2020; Mannino and Balistreri, 2021), where \n",
      "the current MPA network is already highly vulnerable to climate \n",
      "change (Kyprioti et  al., 2021). This new evidence supports SROCC’s \n",
      "high confidence assessment that present governance arrangements, \n",
      "including MPAs, are too fragmented to provide integrated responses \n",
      "to the increasing and cascading risks from climate change in the ocean \n",
      "(SROCC SPMC1.2; IPCC, 2019c).\n",
      "Strategic conservation planning can yield future MPA networks \n",
      "substantially more ready for climate change (e.g., Section  3.6.3.1.5; \n",
      "SROCC SPM C2.1; IPCC, 2019c; Frazão Santos et al., 2020; Rassweiler \n",
      "et  al., 2020).\n"
     ]
    }
   ],
   "source": [
    "print(vector_response.source_nodes[2].node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So AMOC is disuccsed, but sadly it is in the middle chunk. With LLMs, it is often observed that text in the middle of retrieved context is often ignored or less useful. A recent paper [\"Lost in the Middle\" discusses this here](https://arxiv.org/abs/2307.03172)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Evaluation\n",
    "We more rigorously evaluate how well the sentence window retriever works compared to the base retriever.\n",
    "\n",
    "We define/load an eval benchmark dataset and then run different evaluations over it.\n",
    "\n",
    "WARNING: This can be expensive, especially with GPT-4. Use caution and tune the sample size to fit your budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import nest_asyncio\n",
    "import random\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_eval = 30\n",
    "# there are 428 nodes total. Take the first 200 to generate questions (the back half of the doc is all references)\n",
    "sample_eval_nodes = random.sample(base_nodes[:200], num_nodes_eval)\n",
    "# NOTE: run this if the dataset isn't already saved\n",
    "# generate questions from the largest chunks (1024)\n",
    "dataset_generator = DatasetGenerator(\n",
    "    sample_eval_nodes,\n",
    "    llm=OpenAI(model=\"gpt-4\"),\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.save_json(\"data/ipcc_eval_qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "eval_dataset = QueryResponseDataset.from_json(\"data/ipcc_eval_qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    PairwiseComparisonEvaluator,\n",
    ")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: can uncomment other evaluators\n",
    "evaluator_c = CorrectnessEvaluator(llm=OpenAI(model=\"gpt-4\"))\n",
    "evaluator_s = SemanticSimilarityEvaluator()\n",
    "evaluator_r = RelevancyEvaluator(llm=OpenAI(model=\"gpt-4\"))\n",
    "evaluator_f = FaithfulnessEvaluator(llm=OpenAI(model=\"gpt-4\"))\n",
    "# pairwise_evaluator = PairwiseComparisonEvaluator(llm=OpenAI(model=\"gpt-4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    "    get_results_df,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "max_samples = 30\n",
    "\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "\n",
    "# resetup base query engine and sentence window query engine\n",
    "# base query engine\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=2)\n",
    "# sentence window query engine\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "base_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], base_query_engine, show_progress=True\n",
    ")\n",
    "pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], query_engine, show_progress=True\n",
    ")\n",
    "\n",
    "pred_response_strs = [str(p) for p in pred_responses]\n",
    "base_pred_response_strs = [str(p) for p in base_pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "    \"relevancy\": evaluator_r,\n",
    "    \"semantic_similarity\": evaluator_s,\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluations over faithfulness/semantic similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=base_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = get_results_df(\n",
    "    [eval_results, base_eval_results],\n",
    "    [\"Sentence Window Retriever\", \"Base Retriever\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
