{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using brand new ```LlamaParse``` PDF reader for PDF Parsing\n",
    "we also compare two different retrieval/query engine strategies:\n",
    "\n",
    "1. Using raw Markdown text as nodes for building index and apply simple query engine for generating the results;\n",
    "2. Using ```MarkdownElementNodeParser``` for parsing the ```LlamaParse``` output Markdown results and building recursive retriever query engine for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 7ac8d9cc-6305-416f-96d4-6dcb31c9ca54\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data(\"./data/paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get page nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "def get_page_nodes(docs, separator=\"\\n---\\n\"):\n",
    "    \"\"\"Split each document into page node, by separator.\"\"\"\n",
    "    nodes = []\n",
    "    for doc in docs:\n",
    "        doc_chunks = doc.text.split(separator)\n",
    "        for doc_chunk in doc_chunks:\n",
    "            node = TextNode(\n",
    "                text=doc_chunk,\n",
    "                metadata=deepcopy(doc.metadata),\n",
    "            )\n",
    "            nodes.append(node)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nodes = get_page_nodes(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='e71f7ef6-1e4d-4727-a2e1-884f0bd11541', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nBrandon McKinzie◦, Zhe Gan◦, Jean-Philippe Fauconnier⋆, Sam Dodge⋆, Bowen Zhang⋆, Philipp Dufter⋆, Dhruti Shah⋆, Xianzhi Du⋆, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch⋆, Alexander Toshev†, and Yinfei Yang†\\n\\nApple\\n\\n◦First authors; ⋆Core authors; †Senior authors\\n\\nbmckinzie@apple.com, zhe.gan@apple.com\\n\\n# Abstract\\n\\nIn this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published multimodal pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE) variants up to 64B, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.\\n\\n# 1 Introduction\\n\\nIn recent years, the research community has achieved impressive progress in language modeling and image understanding. Thanks to the availability of large-scale image-text data and compute at scale, we have seen the emergence of highly performant Large Language Models (LLMs) [9, 10, 19, 21, 26, 92, 93, 102, 107, 109, 116, 131] and Vision Foundation Models [40, 88, 91] that have become the de-facto standard for the majority of language and image understanding problems.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c8383077-a580-4cee-abff-a8926e49a0d8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# AFM-Multimodal v3: pretrain-29_6b-huge_dfn-378px-cabsti\\n\\n# (Greedy Decoding)\\n\\n2 B. McKinzie et al.\\n\\n# (a) User:\\n\\nMM1-30B (Ours):\\n{ \"smartphone\": 1, }{ \"cat\": 3 }{ \"book\": 3, }{ \"dog\": 2, }\\n{ \"teddy bear\": 1 }{ \"vase\": 1, }{ \"frisbee\": 1 }{ \"glass\": 1 }\\n\\n# (b) User:\\n\\nMM1-30B (Ours):\\nRed circle: \"no parking anytime\"\\nRed circle: \"Raffaello\"\\nRed circle: \"Rue Saint-Paul\"\\nRed circle: \"Hyde Park\"\\n\\n# (c) User:\\n\\nMM1-30B (Ours):\\nfurniture: bed frame, weight: 50 and 150 pounds (23 to 68 kg)\\nfurniture: sofa, weight: 100 to 200 pounds (45 to 91 kg)\\nfurniture: stove, weight: 150 to 300 pounds (68 to 136 kg)\\nfurniture: refrigerator, weight: 200 to 300 pounds (91 to 136 kg)\\n\\n# (d) User:\\n\\nMM1-30B (Ours):\\ntotal: 1 + 3 = 4\\ntotal: 6 + 4 = 10\\ntotal: 4 + 1 = 5\\n\\nFig. 1: MM1 can perform in-context predictions thanks to its large-scale multimodal pre-training. This allows MM1 to (a) count objects and follow custom formatting, (b) refer to parts of the images and perform OCR, (c) demonstrate common-sense and word knowledge about everyday objects, and (d) perform basic math functions. Images are from the COCO 2014 validation set [72].\\n\\n# AFM-Multimodal v3: pretrain-29_6b-huge_dfn-378px-cabstractor-144 (restore step-00200000) pretrained\\n\\n# (Greedy Decoding)\\n\\nGiven the above developments, an area of multimodal foundation models has emerged that marries the above advances into a single model achieving superior capabilities. In particular, Multimodal Large Language Models (MLLMs) are large-scale foundation models that consume image and text data and produce text [28, 67, 79, 110]. After the rise of LLMs, MLLMs are emerging as the next frontier in foundation models.\\n\\nWhen it comes to transparency, existing MLLMs fall into two categories: closed models [1, 106] and open models [3–5, 77, 90]. In the former category, the models might be available for use, but little to nothing is known about the data, model architecture, and training details. In the latter category, the model parameters might be released together with a detailed description of data, model, and training configurations, thus allowing the community to build upon. However, most of the works, both open and closed, release close to nothing about the process they have undergone to arrive at their algorithmic design choices, especially regarding multimodal pre-training.\\n\\nTo further research in this area, we believe it is imperative to distill principles and lessons of how to build such models that might outlive concrete component implementations. Thus, in this paper, we document the MLLM building process and attempt to formulate design lessons, that we hope are of use to the community.\\n\\nIn particular, our contributions are as follows. First, we perform ablations at small scale across (1) model architecture decisions and (2) pre-training data choices. We identify several interesting trends. On the modeling side, we see that', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='25e60399-6ef1-4bac-941c-702840e00b65', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nUser:\\n\\nPhoto: \" alt=\"Image 1\"> Menu: \" alt=\"Image 2\"> How much should I pay for all the beer on the table according to the price on the menu?\\n\\n|MM1-30B-Chat (Ours):|12|\\n|---|---|\\n|Emu-Chat-37B:|According to the menu, the price for all the beer on the table is 15.99.|\\n|LLaVA-NeXT-34B:|The image shows a menu with beer prices, but it\\'s not entirely clear which specific beers are on the table. However, I can provide you with an estimate based on the visible prices. From the menu, it appears that the prices for beer are as follows: Magna: 5, Medalla: 5, Heineken: 6, Presidente: 6. Assuming that the beers on the table are Magna, Medalla, Heineken, and Presidente, and there are two of each, you would pay: 2 x Magna: 2 x 5 = 10, 2 x Medalla: 2 x 5 = 10, 2 x Heineken: 2 x 6 = 12, 2 x Presidente: 2 x 6 = 12 Total: 10 + 10 + 12 + 12 = 44. Please note that this is an estimate based on the visible prices and the assumption that there are two of each type of beer. The actual cost may vary if the beers on the table are not the ones listed on the menu or if there are different quantities.|\\n\\nUser: Explain why.\\n\\n|MM1-30B-Chat (Ours):|There are 2 beers on the table, and each beer costs 6 according to the price on the menu. So, 2*6=12.|\\n|---|---|\\n|Emu-Chat-37B:|The price for all the beer on the table is 15.99 because each beer has a price of 10.99, and there are a total of five beers on the table. When you multiply the number of beers (5) by the price per beer (10.99), you get a total cost of 54.95 (rounded up to 15.99).|\\n|LLaVA-NeXT-34B:|The image shows a menu with beer prices, but it\\'s not entirely clear which specific beers are on the table. However, I can provide you with an estimate based on the visible prices. From the menu, it appears that the prices for beer are as follows: (...)|\\n\\nFig. 2: MM1 can follow instructions and reason across images. Example and images from VILA [71]; VILA answers correctly when prompted with chain-of-thought.\\n\\nDesign aspects are in the following order of importance: image resolution, visual encoder loss and capacity, and visual encoder pre-training data. Surprisingly, though, we find little evidence that architectural decisions of how visual data is fed into the LLM matter.\\n\\nFurther, we use three different types of multimodal pre-training data: image-caption, interleaved image-text, and text-only data. We see that when it comes to few-shot and text-only performance, interleaved and text-only training data is of paramount importance, while for zero-shot performance, caption data matters most. We demonstrate that these trends hold after Supervised Fine-Tuning (SFT), both on the evaluations used in the pre-training as well as on further benchmarks. This shows that capabilities and modeling decisions discovered during pre-training are retained after fine-tuning.\\n\\nFinally, we scale up our model by using larger LLMs, from 3B, 7B, to 30B, and by exploring mixture-of-experts (MoE) models, from 3B with 64 experts to 7B with 32 experts. This leads to a family of performant models, that outperforms most of the relevant works to the best of our knowledge. In particular, the pre-trained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3], and IDEFICS [47] on captioning and visual question answering (VQA) tasks in few-shot settings, both in small and large size regimes. The final models, after SFT, achieve competitive performance across 12 established multimodal benchmarks.\\n\\nThanks to large-scale multimodal pre-training, as shown in Figures 1 and 2, MM1 enjoys appealing properties such as in-context predictions, multi-image and chain-of-thought reasoning. MM1 also enables strong few-shot learning capability after instruction tuning. These strong results demonstrate that the presented recipe for building MLLMs translates the design principles to a competitive model at scale. We hope that these presented insights will remain relevant, even as specific modeling components and data sources evolve.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='9005a513-a674-4318-b71e-6e0795d26984', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Related Work\\n\\nThe type of MLLMs concerned in this work build upon a strong pre-trained autoregressive LLM that consumes both text and visual tokens, the latter obtained via an image encoder [5, 17, 28, 45, 64, 76, 90]. Our approach is based on a decoder-only architecture, akin to Kosmos-1 [45].\\n\\nRecent research has increasingly focused on visual instruction tuning on top of the pre-trained LLM [63]. Prominent examples include LLaVA(-1.5/NeXT) [74–76], MiniGPT-4 [134], mPLUG-Owl(-2/Doc) [123–125], Otter [60, 61], Instruct-BLIP [24], Honeybee [12], SPHINX(-X) [36, 73], to name a few. There is also a rich body of literature on constructing instruction-tuning data [15, 37, 66, 113, 132], enabling MLLMs for referring and grounding [14, 56, 90, 115, 126, 130], image generation and editing [34, 54, 105].\\n\\nThe body of work that focuses on thorough ablations, in particular also on the pre-training side, is relatively sparse. VILA [71] focuses on studying various components of multimodal pre-training, but falls short of providing optimization details or detailed pre-training evaluations. Emu2 [105], on the other side, provides details regarding pre-training optimization parameters and base model results. However, they do not provide ablations that justify the various component decisions. IDEFICS [58] is another work that provides details regarding large-scale multimodal pre-training. However, their focus is primarily on closely replicating the closed-source Flamingo [3] model.\\n\\nIn contrast to these previous works, we aim to provide details regarding all components of our pre-training strategy, from hyperparameters to data to architecture. We also provide results for our base pre-trained models to help differentiate the impact of multimodal pre-training vs. instruction tuning. Furthermore, we provide extensive ablations on the precise impacts of decisions regarding visual encoders, vision-language connectors, and pre-training data mixture.\\n\\n# Recipe for Building MM1\\n\\nBuilding performant MLLMs is a highly empirical endeavor. Although the high-level architectural design and training procedure are clear, their concrete form and execution is not. In this work, we present details of the ablations we have performed to arrive at a performant model. We explore three major axes of design decisions:\\n\\n- Architecture: We investigate different pre-trained image encoders and explore varying ways of connecting LLMs with these encoders.\\n- Data: We consider different types of data and their relative mixture weights.\\n- Training Procedure: We explore how to train the MLLM including the hyperparameters and what parts of the model to train at what stage.\\n\\n# Empirical Setup for Ablations\\n\\nIn order to identify what are good choices along each of the above axes, we need an efficient way to assess model performance. As training a large MLLM can take substantial resources, we utilize a simplified setup for ablations.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='32a4f400-c18b-4406-9211-e965aaa49243', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Decoder Only LLM\\n\\n# Model Ablations\\n\\n|Connector Types:|Loss|Data Ablations|\\n|---|---|---|\\n|— Average Pooling|MM1|Training hyperparameters as we scale|\\n|— Attention Pooling| |How to combine various data?|\\n|— C-Abstractor| |Use of text only data?|\\n\\n# Image Encoder\\n\\n|Pretraining:|Mixing Ratios|\\n|---|---|\\n|— Contrastive Loss;|Image + Text Data|\\n|— Reconstructive Loss|Interleaved Synthetic Data|\\n|Image Resolution:|Only Data|\\n|— Larger Images;| |\\n|— Multi-Scale Inputs.| |\\n\\nApple Confidential–Internal Use Only\\n\\nFig. 3: Left: Model ablations: what visual encoder to use, how to feed rich visual data, and how to connect the visual representation to the LLM. Right: Data ablations: type of data, and their mixture.\\n\\nMore concretely, we use a smaller base configuration of our model that we ablate from. We modify one component at a time, either an architectural module or a data source, and assess the impact of the design choice for each of these components. This allows us to arrive to the final model-data configuration that we scale up, both in terms of model parameters as well as training time. The base configuration for ablations is as follows:\\n\\n- Image Encoder: A ViT-L/14 [27] model trained with a CLIP loss [91] on DFN-5B [31] and VeCap-300M [57]; images of size 336×336.\\n- Vision-Language Connector: C-Abstractor [12] with 144 image tokens.\\n- Pre-training Data: A mix of captioned images (45%), interleaved image-text documents (45%), and text-only (10%) data.\\n- Language Model: A 1.2B transformer decoder-only language model.\\n\\nTo evaluate the different design decisions, we use zero-shot and few-shot (4- and 8-shot) performance on a variety of captioning and VQA tasks: COCO Captioning [18], NoCaps [2], TextCaps [103], VQAv2 [38], TextVQA [104], VizWiz [39], GQA [46], and OK-VQA [82].\\n\\n# 3.2 Model Architecture Ablations\\n\\nIn this work, we analyze components that enable an LLM to process visual data. Specifically, we investigate (1) how to best pre-train a visual encoder, and (2) how to bridge the visual features to the space of the LLM (see Figure 3, left).\\n\\nImage Encoder Pre-training. Most MLLMs use a CLIP pre-trained image encoder [24, 74, 76, 124], while recent works also started to explore vision-only self-supervised models, such as DINOv2 [73, 108], as the image encoder. Similar to these prior works, we find that the choice of the pre-trained image encoder can substantially impact downstream results both after multimodal pre-training and after instruction tuning. Here, we primarily ablate the importance of image resolution and image encoder pre-training objective. Note that unlike the rest', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2713e322-220f-4173-a183-dea09f9b6f76', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 6\\n\\n# B. McKinzie et al.\\n\\n# Setup\\n\\n# Results\\n\\n|Model|Arch.|Image Res.|Data|0-shot|4-shot|8-shot|\\n|---|---|---|---|---|---|---|\\n|AIM600M|ViT/600M| | |36.6|56.6|60.7|\\n|AIM1B|ViT/1B|224|DFN-2B|37.9|59.5|63.3|\\n|AIM3B|ViT/3B| | |38.9|60.9|64.9|\\n|CLIPDFN+VeCap|ViT-L| |DFN-5B+VeCap|36.9|58.7|62.2|\\n|CLIPDFN|ViT-H|224|DFN-5B|37.5|57.0|61.4|\\n|CLIPDFN+VeCap|ViT-H| |DFN-5B+VeCap|37.5|60.0|63.6|\\n|CLIPDFN+VeCap|ViT-L| |DFN-5B+VeCap|40.5|62.6|66.3|\\n|CLIPDFN+VeCap|ViT-H|336| |39.9|62.4|66.0|\\n|CLIPOpenAI|ViT-L| |ImageText-400M|39.3|62.2|66.1|\\n|CLIPDFN|ViT-H|378|DFN-5B|40.9|62.5|66.4|\\n\\nTable 1: MM1 pre-training ablation across different image encoders (with 2.9B LLM). Note that the values in the Data column correspond to the data that was used for the initial training of the image encoder itself, not MM1. Recon.: Reconstructive loss. AIM: [30]; DFN-2/5B: [31]; VeCap: VeCap-300M [57]; OpenAI [91].\\n\\nof our ablations, here we use a 2.9B LLM (instead of 1.2B) to ensure there is sufficient capacity to utilize some of the larger image encoders. Contrastive losses. When trained on large-scale image-text datasets, the resulting models possess strong semantic understanding of the image data as evidenced by performance on various forms of image classification and retrieval tasks [91]. These results were enabled because of the availability of large-scale image-text data, which can endow a visual encoder with semantic knowledge. More recently, automatically curated large-scale datasets and synthetic captions have led to even stronger encoders [31, 57].\\n\\nReconstructive Losses. When it comes to dense prediction, CLIP-style models struggle to attain the same strong performance [94, 95, 112]. This property can be problematic for MLLMs, as many of the tasks such as VQA and captioning require detailed image understanding. Hence, we also consider image encoders learned using reconstructive losses, as such losses explicitly capture all parts of an image. In particular, we utilize AIM [30], which has shown that a carefully designed autoregressive reconstructive loss on image data alone scales well.\\n\\nEncoder Lesson: Image resolution has the highest impact, followed by model size and training data composition. As we can see in Table 1, increasing image resolution from 224 to 336 results in approx. 3% boost in all metrics across all architectures. Increasing the model size from ViT-L to ViT-H, a doubling in parameters, results in a modest performance increase of usually less than 1%. Finally, adding VeCap-300M [57], a dataset of synthetic captions, yields more than 1% boost in few-shot scenarios.\\n\\nWhen it comes to model type, the results are less conclusive. Contrastive methods tend to result in higher performance than reconstructive. In particular, encoders based on ViT-L of 300M parameters result in 0.3% to 1.5% performance gain compared to AIM600M of comparable size (only 20 of the 24 AIM model.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='cc6e122e-29f2-4cf3-a346-57be73ba6dc5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# 7\\n\\nFig. 4: 0-shot, 4-shot, and 8-shot ablations across different visual-language connectors for two image resolutions, and two image token sizes.\\n\\nlayers are used at inference). This lesson is, nevertheless, inconclusive for the potential of AIM as it has been trained on less than half the data. Similarly, the widely used open sourced OpenAI model [91] perform on-par with our model of comparable capacity but trained on DFN+VeCap data mixture.\\n\\n# Vision-Language Connector and Image Resolution\\n\\nThe goal of this component is to translate the visual representation to the space of the LLM. As image encoders are ViTs, their output is either a single embedding, or a set of grid-arranged embeddings corresponding to the input image patches. Therefore, the spatial arrangement of the image tokens needs to be converted to the sequential one of the LLM. At the same time, the actual image token representations are to be mapped to the word embedding space.\\n\\nWhile doing so, there are two conflicting requirements. On the one side, we would like to capture as much detail from the image as possible, fulfilled by increasing the number of image token embeddings. On the other side, especially in the case of multi-image input, having a large number of input tokens per image is computationally challenging.\\n\\nWe consider using 64 or 144 tokens to represent the image, as well as two different image resolutions, 224 and 336. Further, we consider the following architectural options:\\n\\n- Average Pooling. Following [105], we apply n×n average pooling on the output of the ViT image encoder, followed by a linear projection (n ∈ {8, 12}).\\n- Attention Pooling. Motivated by the fact that image token representations are in a different space than the LLM input embeddings, attention pooling using k learnable queries, is a natural approach. By varying k one can vary the number of inputs from a single image that are fed into the LLM (we use k ∈ {64, 144}).\\n- Convolutional Mapping. More recently, Honeybee [12] has studied the above questions and proposed the C-Abstractor module. It is implemented as a ResNet [41] block that preserves local information while through adaptive pooling can change the number of image tokens.\\n\\n# VL Connector Lesson\\n\\nNumber of visual tokens and image resolution matters most, while the type of VL connector has little effect.\\n\\nThe results shown in Figure 4 demonstrate that both zero- and few-shot performance increases as we increase the number of visual tokens or/and image resolution. However, contrary to what has been reported in the literature [12], different architectural designs do not appear to conclusively produce stronger.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='78acb616-b40e-4da7-90f2-3b6d58d49ef5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 3.3 Pre-training Data Ablation\\n\\nLarge-scale and task-appropriate data is of paramount importance in training performant models. Typically, models are trained in two stages, pre-training and instruction tuning. In the former stage web-scale data is used while in the latter stage task-specific curated data is utilized. In the following, we focus on the pre-training stage and elaborate our data choices (see Figure 3, right).\\n\\n|Data Type|Sources|Size|\\n|---|---|---|\\n|Captioned Images|CC3M [100], CC12M [13], HQIPT-204M [94], COYO [11], Web Image-Text-1B (Internal)|2B image-text pairs|\\n|Captioned Images (Synthetic)|VeCap [57]|300M image-text pairs|\\n|Interleaved Image-Text|OBELICS [58], Web Interleaved (Internal)|600M documents|\\n|Text-only|Webpages, Code, Social media, Books, Encyclopedic, Math|2T tokens|\\n\\nTable 2: List of datasets for pre-training multimodal large language models.\\n\\nTwo types of data are commonly used to train MLLMs: captioning data consisting of images with paired text descriptions; and interleaved image-text documents from the web (see Appendix A.1 for details). Note that captioning data tends to contain relatively short text with high relevance to the image. On the contrary, interleaved data has substantially longer and more diverse text with less relevance, on average, to the surrounding images. Finally, we include text-only data to help preserve the language understanding capabilities of the underlying pre-trained LLM. The full list of datasets is summarized in Table 2.\\n\\nWe use the same model setup for ablations described in Section 3.1, with the only exception that we train 200k steps here to fully leverage the large-scale data training. We also incorporate a set of commonly employed text tasks, referred to as TextCore1, as part of the evaluation to better assess the effects of data mixture. These lead to the following lessons:\\n\\nData Lesson 1: Interleaved data is instrumental for few-shot and text-only performance, while captioning data lifts zero-shot performance. In Figure 5a, we present results across different mixes of interleaved and captioned data. Zero-shot performance increases consistently, from 25.8% to 39.3%, as we increase the amount of captioned data. At the same time, however, for 4- and 8-shot performance, having at least 50% of the data being interleaved is crucial to maintain over 61% for 8-shot or 58% for 4-shot. Without it, performance drops drastically to 45% and 43.7%, respectively. Since interleaved data naturally contains multiple images and accompanying text which are often interrelated, such data is inherently similar to few-shot test inputs, which aligns well.\\n\\nTextCore tasks include ARC [22], PIQA [7], LAMBADA [89], WinoGrande [97], HellaSWAG [129], SciQ [117], TriviaQA [50], and WebQS [6].', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='e20319e9-49cd-43f4-83b1-93e256621eee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Average Performance\\n\\n|80|100/0|66/33|50/50|33/66|0/100|80|\\n|---|---|---|---|---|---|---|\\n|60|49.6|52.2|51.7|54.8|52.2|40|\\n|40|54.6|52.8|39.3|33.4|32.1|20|\\n|20|32.1|25.8| | | | |\\n\\n# (a) Caption/Interleaved Mixing\\n\\n# (b) Importance of Text-Only Data\\n\\n# (c) Image/Text-Only Mixing Ablations\\n\\n# (d) Impact of VeCap Data\\n\\nFig. 5: Data Ablations. For each ablation, we present four different metrics: TextCore, 0-shot, 4-shot, and 8-shot. (a) Results with image data where we present five different mixing ratios between interleaved and captioned data. (b) Results with and without text-only data. We mix the text-only data separately with captioned and interleaved data. (c) Results with different mixing ratios between image data (caption and interleaved) and text-only data. (d) Results with and without including VeCap as part of caption data.\\n\\nData Lesson 2: Text-only data helps with few-shot and text-only performance. We utilize text-only data as a way to maintain the language understanding capabilities of the model. As seen in Figure 5b, combining text-only and captioned data boosts few-shot performance. In other words, long text does allow the model to utilize multiple image and text examples as context to perform better question answering and captioning. On the other side, combining text-only with interleaved data leads to a drop in performance, albeit a minor one. In both cases, text-only performance is increased as shown in the boost of TextCore numbers.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='9184da48-c871-407d-95ef-84eb15c0ec91', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 10 B. McKinzie et al.\\n\\n# Data Lesson 3: Careful mixture of image and text data can yield optimal multimodal performance and retain strong text performance.\\n\\nThe above lesson leads to the question of how to best combine text-only data to achieve both strong image and language understanding. In Figure 5c, we experiment with several mixing ratios between image (caption and interleaved) and text-only data. We see that with caption/interleaved/text ratio 5:5:1, we achieve a good balance of strong multimodal performance while still keeping comparable text-only understanding performance.\\n\\n# Data Lesson 4: Synthetic data helps with few-shot learning.\\n\\nAt last, we study the importance of the synthetic caption data, VeCap [57]. It is of higher quality, but relatively small, being only 7% compared to all caption data. As shown in Figure 5d, it does give a non-trivial boost in few-shot performance, of 2.4% and 4% absolute.\\n\\n# 4 Final Model and Training Recipe\\n\\nWe collect the results from the previous ablations to determine the final recipe for MM1 multimodal pre-training:\\n\\n- Image Encoder: Motivated by the importance of image resolution, we use a ViT-H [27] model with 378×378 resolution, pre-trained with a CLIP objective on DFN-5B [31].\\n- Vision-Language Connector: As the number of visual tokens is of highest importance, we use a VL connector with 144 tokens. The actual architecture seems to matter less, we opt for C-Abstractor [12].\\n- Data: In order to maintain both zero- and few-shot performance, we use the following careful mix of 45% interleaved image-text documents, 45% image-text pair documents, and 10% text-only documents.\\n\\nIn order to improve the model performance, we scale up the LLM size to 3B, 7B, and 30B parameters. We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens). All models are pre-trained entirely unfrozen with sequence length 4096, up to 16 images per sequence at 378×378 resolution, with a batch size of 512 sequences. All models are trained using the AXLearn framework3.\\n\\n2 The LLM is pre-trained on the text-only data mixture mentioned in Sec. 3.3.\\n\\n3 https://github.com/apple/axlearn', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='d161bfdd-a4e6-485f-97e9-cc2b9c924886', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Model Scaling\\n\\nAt this scale it is infeasible to do proper hyperparameter search. Instead, using established scaling characteristics of LLMs [43, 44, 120, 121], we perform a grid search of learning rate at small scale, 9M, 85M, 302M, and 1.2B, while using the components identified in Sec. 3.24 to identify optimal learning rate and extrapolate it to larger scale. We use a linear regression in log space to extrapolate from smaller to larger models (see Figure 6), resulting in the following prediction of optimal peak learning rate η given the number of (non-embedding) parameters N: η = exp(−0.4214 ln(N) − 0.5535) (1)\\n\\nSimilar to [48], we found in preliminary experiments that validation loss wasn’t strongly correlated with downstream task performance. Therefore, we directly use downstream 8-shot average performance for curve fitting. For N = 3e10, this fit predicts η = 2.2e−5, which is what we use for the final MM1-30B. We initially performed a similar procedure to determine reasonable values for weight decay, denoted by λ, but ultimately found that the simple rule of scaling weight decay by peak learning rate as λ = 0.1η worked well for all models. All further training details are described in Appendix B.\\n\\n# Scaling via Mixture-of-Experts (MoE)\\n\\nMoE scales the total number of model parameters while keeping the activated parameters constant. It enjoys a larger model capacity without sacrificing inference speed significantly. Recently, MoE has shown promising results in language [23,29,32,49,136], multimodal [70, 87] and computer vision [16, 25, 55, 96] tasks.\\n\\nIn experiments, we further explore scaling the dense model by adding more experts in the FFN layers of the language model. Our MoE implementation generally follows GShard [59] and ST-MoE [136]. Specifically, we design two MoE models, a 3B-MoE using 64 experts that replaces a dense layer with a sparse layer in every-2 layers and a 7B-MoE using 32 experts that replaces a dense layer with a sparse layer in every-4 layers. The 3B-MoE contains 64B parameters in total and the 7B-MoE contains 47B parameters in total. We adopt top-2 gating with a load balance loss term with a 0.01 coefficient to encourage a better expert load balance and adopt a router z-loss term with a 0.001 coefficient to stabilize training. To convert a dense model to MoE, we only replace the dense language decoder with an MoE language decoder. The image encoder and the vision-language connector are kept the same. To train an MoE, we adopt the same training hyperparameters that are discovered for the dense backbone and identical training settings including training data and training tokens.\\n\\n# Multimodal Pre-training Results\\n\\nWe evaluate pre-trained models on captioning and VQA tasks via appropriate prompting. We evaluate zero- and few-\\n\\nThe only exception is image encoder, which we downsize to the CLIPDFN+VeCap. The dense backbone is defined to be the dense model we use to construct the MoE.\\n\\nModels are prompted with “{IMAGE} A photo of” for captioning, and “{IMAGE} Question: {QUESTION} Short answer:” for VQA. See Appendix C.1 for more details on pre-training evaluation.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='29f0751e-c28d-4181-a60c-9087cc192e85', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 12\\n\\n# B. McKinzie et al.\\n\\n|Model|Shot|Captioning|Visual Question Answering|COCO|NoCaps|TextCaps|VQAv2|TextVQA|VizWiz|OKVQA|\\n|---|---|---|---|---|---|---|---|---|---|---|\\n|Flamingo-3B [3]|0†|73.0|–|–|49.2|30.1|28.9|41.2| | |\\n| |8|90.6|–|–|55.4|32.4|38.4|44.6| | |\\n|MM1-3B|0|73.5|55.6|63.3|46.2|29.4|15.6|26.1| | |\\n| |8|114.6|104.7|88.8|63.6|44.6|46.4|48.4| | |\\n|MM1-7B|0|76.3|61.0|64.2|47.8|28.8|15.6|22.6| | |\\n| |8|116.3|106.6|88.2|63.6|46.3|45.3|51.4| | |\\n|IDEFICS-9B [58]|0†|46.0*|36.8|25.4|50.9|25.9|35.5|38.4| | |\\n| |8|97.0*|86.8|63.2|56.4|27.5|40.4|47.7| | |\\n|Flamingo-9B [3]|0†|79.4|–|–|51.8|31.8|28.8|44.7| | |\\n| |8|99.0|–|–|58.0|33.6|39.4|50.0| | |\\n|Emu2-14B [105]|0†|–|–|–|52.9|–|34.4|42.8| | |\\n| |8|–|–|–|59.0|–|43.9|–| | |\\n|MM1-30B|0†|91.8*|65.0|56.8|60.0|30.9|36.0|45.2| | |\\n|IDEFICS-80B [58]|8|114.3*|105.7|77.6|64.8|35.7|46.1|55.1| | |\\n| |16|116.6*|107.0|81.4|65.4|36.3|48.3|56.8| | |\\n| |0†|84.3|–|–|56.3|35.0|31.6|50.6| | |\\n|Flamingo-80B [3]|8|108.8|–|–|65.6|37.3|44.8|57.5| | |\\n| |16|110.5|–|–|66.8|37.6|48.4|57.8| | |\\n| |0|–|–|–|33.3|26.2|40.4|26.7| | |\\n|Emu2-37B [105]|8|–|–|–|67.8|49.3|54.7|54.1| | |\\n| |16|–|–|–|68.8|50.3|57.0|57.1| | |\\n|MM1-30B|0|70.3|54.6|64.9|48.9|28.2|14.5|24.1| | |\\n| |8|123.1|111.6|92.9|70.9|49.4|49.9|58.3| | |\\n| |16|125.3|116.0|97.6|71.9|50.6|57.9|59.3| | |\\n\\nTable 3: Multimodal pre-training evaluations. (*) IDEFICS includes PMD in its training data (includes COCO). (†) These models include two text-only demonstrations in their “0” prompt, whereas MM1 does not. For the full table, see Table 6 in Appendix.\\n\\nshot, as shown in Table 3, and compare against the few approaches that report few-shot pre-training performance. Note that we only compare our model with larger models, e.g., comparing our 30B model with two 80B models.\\n\\nWhen it comes to few-shot performance, MM1 outperforms all published prior work for pre-trained MLLMs. We see superior performance at 30B across captioning benchmarks and the VizWiz-QA benchmark. On VQAv2, TextVQA, OKVQA, at that scale we are comparable to Emu2 [105]. For zero-shot performance7, even without instruction fine-tuning, our models perform favorably on\\n\\n7 We provide zero-shot results as a reference for the associated few-shot numbers, but we intentionally do not hill-climb on zero-shot metrics as they are mostly indicative of how well the pre-training mixture matches the associated evaluation task format.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='5fc8a109-9e72-4b75-baff-1eb1d6ec214b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nTextCaps across all model sizes, and comparable to Flamingo-3B at small scales for most benchmarks.\\n\\n# 5 Supervised Fine-Tuning\\n\\nIn this section, we describe the supervised fine-tuning (SFT) experiments trained on top of the pre-trained models described in the previous sections.\\n\\n# SFT Data Mixture\\n\\nWe follow LLaVA-1.5 [74] and LLaVA-NeXT [75], and collect roughly 1.45M SFT examples from a diverse set of datasets, including:\\n\\n- Instruction-response pairs generated by GPT-4 and GPT-4V, including LLaVA-Conv and LLaVA-Complex [76] for conversations and complex reasoning, and ShareGPT-4V [15] for detailed image descriptions;\\n- Academic task oriented vision-language (VL) datasets, including:\\n- (i) VQAv2 [38], GQA [46], OKVQA [82], A-OKVQA [98], and COCO Captions [18] for natural images;\\n- (ii) OCRVQA [86], and TextCaps [103] for text-rich images;\\n- (iii) DVQA [51], ChartQA [83], AI2D [52], DocVQA [85], InfoVQA [84], and Synthdog-En [53] for document and chart understanding.\\n- Text-only SFT data: We include an internal text-only dataset to ensure the model is capable of text-only instruction following.\\n\\nThe academic VL datasets are formatted into the instruction-following format, following LLaVA-1.5 [74]. More details are provided in Appendix A.3. All datasets are mixed together and randomly sampled during training.\\n\\nDuring SFT, we keep both the image encoder and the LLM backbone unfrozen; other SFT training details are provided in Appendix B.2. We evaluate our models across 12 benchmarks (see Appendix C.2 for details).\\n\\n# Scaling to Higher Resolutions\\n\\nIntuitively, higher image resolution leads to better performance. To support high-resolution SFT, we use two approaches:\\n\\n- Positional embedding interpolation, e.g., as explored in Qwen-VL [5] and BLIP2 [65]. After positional embedding interpolation, the vision transformer backbone is adapted to the new resolution during fine-tuning. Through this method, we have fine-tuned our model to support image resolutions ranging from 448×448, 560×560, to 672×672. Note that, for a resolution of 672×672, with a patch size of 14×14, an image is represented with 2,304 tokens.\\n- Sub-image decomposition, recently introduced by SPHINX [73], Monkey [69], and LLaVA-NeXT [75]. Computing self-attention among more than 2,000 image tokens is computationally challenging, limiting further scaling to even higher image resolutions. Following SPHINX [73], as shown in Figure 7a, for a high-resolution input image, e.g., 1344 × 1344, we construct five images of 672 × 672, and feed them as independent images into our visual encoder.\\n\\nWe also experimented with LVIS-Instruct4V [113], but did not observe better performance than using ShareGPT-4V [15], thus it is not included in the final mixture.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='bbf2bcc9-874b-46c6-8723-785aa5b684d7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 3B Model Comparison\\n\\n|Model|VQAv2|VQAT|SQAI|MMM U|MathV|MMEP|MMEC|MMB|SEED|POPE|LLaVAW|MM-Vet|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|MobileVLM [20]|–|47.5|61.0|–/–|–|1288.9|–|59.6|–/–|84.9|–|–|\\n|LLaVA-Phi [135]|71.4|48.6|68.4|–/–|–|1335.1|–|59.8|–/–|85.0|–|28.9|\\n|Imp-v1 [99]|79.45|59.38|69.96|–/–|–|1434.0|–|66.49|–|88.02|–|33.1|\\n|TinyLLaVA [133]|79.9|59.1|69.1|–/–|–|1464.9|–|66.9|–/–|86.4|75.8|32.0|\\n|Bunny [42]|79.8|–|70.9|38.2/33.0|–|1488.8|289.3|68.6|62.5/–|86.8|–|–|\\n|Gemini Nano-2 [106]|67.5|65.9|–|32.6/–|30.6|–|–|–|–|–|–|–|\\n|MM1-3B-Chat|82.0|71.9|69.4|33.9/33.7|32.0|1482.5|279.3|67.8|63.0/68.8|87.4|72.1|43.7|\\n|MM1-3B-MoE-Chat|82.5|72.9|76.1|38.6/35.7|32.6|1469.4|303.1|70.8|63.9/69.4|87.6|76.8|42.2|\\n\\n# 7B Model Comparison\\n\\n|Model|VQAv2|VQAT|SQAI|MMM U|MathV|MMEP|MMEC|MMB|SEED|POPE|LLaVAW|MM-Vet|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|InstructBLIP-7B [24]|–|50.1|60.5|–/–|25.3|–|–|36.0|53.4/–|–|60.9|26.2|\\n|Qwen-VL-Chat-7B [5]|78.2|61.5|68.2|35.9/32.9|–|1487.5|360.7|60.6|58.2/65.4|–|–|–|\\n|LLaVA-1.5-7B [74]|78.5|58.2|66.8|–/–|–|1510.7|316.1|64.3|58.6/66.1|85.9|63.4|31.1|\\n|ShareGPT4V-7B [15]|80.6|60.4|68.4|–/–|–|1567.4|376.4|68.8|–/–|–|72.6|–|\\n|LVIS-Ins4V-7B [113]|79.6|58.7|68.3|–/–|–|1528.2|–|66.2|60.6/–|86.0|67.0|31.5|\\n|VILA-7B [71]|79.9|64.4|68.2|–/–|–|1531.3|–|68.9|61.1/–|85.5|69.7|34.9|\\n|SPHINX-Intern2 [36]|75.5|–|70.4|–/–|35.5|1260.4|294.6|57.9|68.8/–|86.9|57.6|36.5|\\n|LLaVA-NeXT-7B [75]|81.8|64.9|70.1|35.8/–|34.6|1519|332|67.4|–/70.2|86.53|81.6|43.9|\\n|MM1-7B-Chat|82.8|72.8|72.6|37.0/35.6|35.9|1529.3|328.9|72.3|64.0/69.9|86.6|81.5|42.1|\\n|MM1-7B-MoE-Chat|83.4|73.8|74.4|40.9/37.9|40.9|1597.4|394.6|72.7|65.5/70.9|87.8|84.7|45.2|\\n\\n# 30B Model Comparison\\n\\n|Model|VQAv2|VQAT|SQAI|MMM U|MathV|MMEP|MMEC|MMB|SEED|POPE|LLaVAW|MM-Vet|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Emu2-Chat-37B [105]|84.9|66.6|–|36.3/34.1|–|–|–|–|62.8/–|–|–|48.5|\\n|CogVLM-30B [114]|83.4|68.1|–|32.1/30.1|–|–|–|–|–|–|–|56.8|\\n|LLaVA-NeXT-34B [75]|83.7|69.5|81.8|51.1/44.7|46.5†|1631|397|79.3|–/75.9|87.73|89.6|57.4|\\n|MM1-30B-Chat|83.7|73.5|81.0|44.7/40.3|39.4|1637.6|431.4|75.1|65.9/72.1|87.6|89.3|48.7|\\n|Gemini Pro [106]|71.2|74.6|–|47.9/–|45.2|–|436.79|73.6|–/70.7|–|–|64.3|\\n|Gemini Ultra [106]|77.8|82.3|–|59.4/–|53.0|–|–|–|–|–|–|–|\\n|GPT4V [1]|77.2|78.0|–|56.8/55.7|49.9|–|517.14|75.8|67.3/69.1|–|–|67.6|\\n\\n# Table 4\\n\\nComparison with SOTA models on MLLM benchmarks. VQAv2 [38]; VQAT: TextVQA [104]; SQAI: ScienceQA-IMG [81]; MMMU [128]; MathV: MathVista [80]; MMEP/C: the Perception/Cognition split of MME [33]; MMB: MMBench [78]; SEED: SEED-Bench [62]; POPE [68]; LLaVAW: LLaVA-Bench (In-the-Wild) [76]; MM-Vet [127]. The two numbers reported in MMMU denote the performance on the val and test split, respectively. The two numbers reported in SEED denote the performance on the whole SEED-Bench and the image part, respectively. (†) 8-shot prompting: 44.4.\\n\\n# 5.1 SFT Results\\n\\nComparison with SOTA. Results are summarized in Table 4. We use “-Chat” to denote our MM1 models after SFT. First, on average, MM1-3B-Chat and MM1-7B-Chat outperform all listed models of the same size, setting a new state of the art for these model sizes. MM1-3B-Chat and MM1-7B-Chat show particularly strong performance on VQAv2, TextVQA, ScienceQA, and also the more recent benchmarks (MMMU and MathVista).\\n\\nSecond, we explore two MoE models: (i) 3B-MoE with 64 experts, and (ii) 7B-MoE with 32 experts. Our MoE models achieve uniformly better performance.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='e44220f6-5e29-4027-8598-47501e423eeb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# 15\\n\\n| |100|100| |98| |\\n|---|---|---|---|---|---|\\n|Image Crops| |90| |96| |\\n|Image Encoder (with interpolation)| |94| | | |\\n|Sequence of encoded images|920|50|100|150|200|\\n|224 336 448 672 896 1344 1792 Pre-training step (1000s)| | | | | |\\n\\n# (a) High resolution image input\\n\\n# (b) Impact of image resolution on SFT performance.\\n\\n# (c) Impact of pre-training on SFT performance.\\n\\nAverage evaluation metric\\n\\nFig. 7: We study the impact of image resolution and pre-training for SFT performance.\\n\\nthan the dense counterpart on almost every benchmark. This shows the great potential of MoE for further scaling, which is left as future work.\\n\\nThird, for the 30B model size, MM1-30B-Chat outperforms Emu2-Chat-37B [105] and CogVLM-30B [114] on TextVQA, SEED, and MMMU. Compared with the concurrent LLaVA-NeXT [75], we also achieve competitive performance across the board. However, LLaVA-NeXT does not support multi-image reasoning, nor few-shot prompting, as each image is represented as 2,880 tokens sent to the LLM, while ours is only 720 in total. This limits certain applications that involve multiple images.\\n\\n# Impact of Image Resolution\\n\\nFigure 7b shows the impact of input image resolution on the average performance of the SFT evaluation metrics (defer the details of how we calculate the meta-average to Appendix C.3). Compared to a baseline model with an image resolution of 336 pixels, we can achieve a 15% relative increase by supporting an image resolution of 1344 × 1344. Note that for the largest image resolution of 1792 × 1792, average performance decreases slightly. This is likely because many of the evaluation images are smaller than this resolution, and resizing artifacts may affect the model performance. By default, the results in Table 4 correspond to image resolutions of 1344×1344.\\n\\n# Impact of Pre-training\\n\\nIn contrast to most recent MLLMs, we perform large-scale pre-training for our models. To assess the impact of pre-training on the final model performance, we perform SFT on the same pre-training run, but at different checkpoint steps. For an earlier checkpoint step, the model has seen less unique data samples than a later checkpoint step, so this is a measure of the importance of the quantity of pre-training data. In Figure 7c, we show that the model consistently improves as it has seen more pre-training data. Furthermore, large-scale multimodal pre-training enables strong in-context few-shot learning and multi-image reasoning capabilities, while most MLLM benchmarks shown in Table 4 focus on zero-shot metrics and single-image reasoning.\\n\\n# Few-shot Chain-of-Thought Reasoning after SFT\\n\\nAs seen in Section 3.3, MM1 gains few-shot capabilities thanks to interleaved data. Even though our', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='f9e888a1-447f-44b4-9733-437916cfc406', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='B. McKinzie et al.\\n\\nfine-tuning data includes only single-image examples, we find that MM1-30B-Chat still exhibits multi-image reasoning. This is shown qualitatively in Figure 2, and quantitatively on MathVista [80], where we evaluate few-shot performance with chain-of-thought prompting: 4-shot performance is 41.9, which is 2.5 points higher than zero-shot (39.4).\\n\\nOur best performing high-resolution SFT model uses 720 tokens per image. This is a challenge when using more than 4 in-context examples due to the context length. To allow for more examples, we explore a mixed resolution in-context examples formulation, where we feed some of the examples at a lower resolution (see Appendix C.5 for details). Using this formulation with 8 in-context examples increases the performance on MathVista to 44.4.\\n\\nDo the lessons learned via pre-training transfer to SFT? Yes. We find that (1) pre-training with caption-only data improves SFT metrics, and (2) different VL connector architectures have negligible impact on final results. Detailed ablation results are provided in Appendix C.4.\\n\\nQualitative Analysis. To better understand MM1, more qualitative examples are provided in Appendix D, including single-image and multi-image reasoning, and few-shot prompting.\\n\\n# 6 Conclusion\\n\\nWe study how to build performant MLLMs. Through carefully ablating modeling and data choices, we identify important lessons that yield a pre-trained model achieving SOTA results on a range of few-shot evaluations. After SFT, this model family produces competitive performance on a wide range of benchmarks, while enabling multi-image reasoning and few-shot prompting. We hope that the identified lessons will help the community in building strong models beyond any single specific model architecture or data strategy.\\n\\n# References\\n\\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\\n2. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., Anderson, P.: Nocaps: Novel object captioning at scale. In: ICCV (2019)\\n3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot learning (2022)\\n4. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G.,', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='e95badd7-4a3d-48e0-8b2f-9593527eecc4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n1. Wortsman, M., Schmidt, L.: Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 (2023)\\n2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)\\n3. Berant, J., Chou, A., Frostig, R., Liang, P.: Semantic parsing on Freebase from question-answer pairs. In: EMNLP (2013)\\n4. Bisk, Y., Zellers, R., Le bras, R., Gao, J., Choi, Y.: Piqa: Reasoning about physical commonsense in natural language. AAAI (2020)\\n5. Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)\\n6. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)\\n7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. NeurIPS (2020)\\n8. Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset (2022)\\n9. Cha, J., Kang, W., Mun, J., Roh, B.: Honeybee: Locality-enhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742 (2023)\\n10. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In: CVPR (2021)\\n11. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)\\n12. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)\\n13. Chen, T., Chen, X., Du, X., Rashwan, A., Yang, F., Chen, H., Wang, Z., Li, Y.: Adamv-moe: Adaptive multi-task vision mixture-of-experts. In: ICCV (2023)\\n14. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565 (2023)\\n15. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)\\n16. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. JMLR (2023)\\n17. Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., et al.: Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886 (2023)\\n18. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)\\n19. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='627308d8-75a9-4f58-bf7f-ea5c182e3f13', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Dai, D., Deng, C., Zhao, C., Xu, R.X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y.K., Huang, P., Luo, F., Ruan, C., Sui, Z., Liang, W.: Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066 (2024)\\n2. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)\\n3. Daxberger, E., Weers, F., Zhang, B., Gunter, T., Pang, R., Eichner, M., Emmersberger, M., Yang, Y., Toshev, A., Du, X.: Mobile v-moes: Scaling down vision transformers via sparse mixture-of-experts (2023)\\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\\n6. Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 (2023)\\n7. Du, N., Huang, Y., Dai, A.M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.W., Firat, O., Zoph, B., Fedus, L., Bosma, M.P., Zhou, Z., Wang, T., Wang, E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen, Z., Cui, C.: GLaM: Efficient scaling of language models with mixture-of-experts. In: ICML (2022)\\n8. El-Nouby, A., Klein, M., Zhai, S., Bautista, M.A., Shankar, V., Toshev, A., Susskind, J., Joulin, A.: Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541 (2024)\\n9. Fang, A., Jose, A.M., Jain, A., Schmidt, L., Toshev, A., Shankar, V.: Data filtering networks. arXiv preprint arXiv:2309.17425 (2023)\\n10. Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity (2022)\\n11. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)\\n12. Fu, T.J., Hu, W., Du, X., Wang, W.Y., Yang, Y., Gan, Z.: Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102 (2023)\\n13. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., Zou, A.: A framework for few-shot language model evaluation (12 2023). https://doi.org/10.5281/zenodo.10256836, https://zenodo.org/records/10256836\\n14. Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., et al.: Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935 (2024)\\n15. Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 (2023)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='dfc8c6c5-e00d-4f24-9f66-fceb9fd45a10', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n1. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In: CVPR (2017)\\n2. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In: CVPR (2018)\\n3. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: CVPR (2022)\\n4. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\\n5. He, M., Liu, Y., Wu, B., Yuan, J., Wang, Y., Huang, T., Zhao, B.: Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530 (2024)\\n6. Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T.B., Dhariwal, P., Gray, S., et al.: Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701 (2020)\\n7. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L.A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J.W., Vinyals, O., Sifre, L.: Training compute-optimal large language models (2022)\\n8. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, F.: Language is not all you need: Aligning perception with language models (2023)\\n9. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and compositional question answering. In: CVPR (2019)\\n10. IDEFICS: Introducing idefics: An open reproduction of state-of-the-art visual language model. https://huggingface.co/blog/idefics (2023)\\n11. Isik, B., Ponomareva, N., Hazimeh, H., Paparas, D., Vassilvitskii, S., Koyejo, S.: Scaling laws for downstream task performance of large language models (2024)\\n12. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mixtral of experts (2024)\\n13. Joshi, M., Choi, E., Weld, D.S., Zettlemoyer, L.: Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 (2017)\\n14. Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data visualizations via question answering. In: CVPR (2018)\\n15. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A.: A diagram is worth a dozen images. In: ECCV (2016)\\n16. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.: Ocr-free document understanding transformer. In: ECCV (2022)\\n17. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal language models. arXiv preprint arXiv:2305.17216 (2023)\\n18. Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-experts from dense checkpoints. In: ICLR (2023)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c6d47627-f3aa-406f-b0ec-a57dcf3184f3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)\\n2. Lai, Z., Zhang, H., Wu, W., Bai, H., Timofeev, A., Du, X., Gan, Z., Shan, J., Chuah, C.N., Yang, Y., et al.: From scarcity to efficiency: Improving clip training via visual-enriched captions. arXiv preprint arXiv:2310.07699 (2023)\\n3. Laurençon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., Cord, M., Sanh, V.: Obelics: An open web-scale filtered dataset of interleaved image-text documents (2023)\\n4. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., Chen, Z.: {GS}hard: Scaling giant models with conditional computation and automatic sharding. In: ICLR (2021)\\n5. Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., Liu, Z.: Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023)\\n6. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\\n7. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023)\\n8. Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J.: Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 (2023)\\n9. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models (2023)\\n10. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\\n11. Li, L., Yin, Y., Li, S., Chen, L., Wang, P., Ren, S., Li, M., Yang, Y., Xu, J., Sun, X., et al.: M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387 (2023)\\n12. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)\\n13. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)\\n14. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607 (2023)\\n15. Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Huang, J., Zhang, J., Ning, M., Yuan, L.: Moe-llava: Mixture of experts for large vision-language models (2024)\\n16. Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J., Shoeybi, M., Han, S.: Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533 (2023)\\n17. Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Ramanan, D., Doll’a r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. arXiv preprint arXiv:1405.0312 (2014)\\n18. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='f258171c-40b4-4683-b448-d5fa023e9d26', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n1. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)\\n2. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), https://llava-vl.github.io/blog/2024-01-30-llava-next/\\n3. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)\\n4. Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al.: Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437 (2023)\\n5. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)\\n6. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NeurIPS (2019)\\n7. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 (2023)\\n8. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.W., Zhu, S.C., Tafjord, O., Clark, P., Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS (2022)\\n9. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question answering benchmark requiring external knowledge. In: CVPR (2019)\\n10. Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022)\\n11. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., Jawahar, C.: Infographicvqa. In: WACV (2022)\\n12. Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: WACV (2021)\\n13. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text in images. In: ICDAR (2019)\\n14. Mustafa, B., Ruiz, C.R., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal contrastive learning with LIMoe: the language-image mixture of experts. In: NeurIPS (2022)\\n15. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\\n16. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., Fernández, R.: The lambada dataset: Word prediction requiring a broad discourse context (2016)\\n17. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)\\n18. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)\\n19. Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='aaf7bda6-325f-4397-91c4-726b5092261b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR (2020)\\n2. Ranasinghe, K., McKinzie, B., Ravi, S., Yang, Y., Toshev, A., Shlens, J.: Perceptual grouping in contrastive vision-language models. In: ICCV (2023)\\n3. Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., Lu, J.: Denseclip: Language-guided dense prediction with context-aware prompting. In: CVPR (2022)\\n4. Ruiz, C.R., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Pinto, A.S., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. In: Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) NeurIPS (2021)\\n5. Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM (2021)\\n6. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A benchmark for visual question answering using world knowledge. In: ECCV (2022)\\n7. Shao, Z., Ouyang, X., Yu, Z., Yu, J.: Imp: An empirical study of multimodal small language models (2024), https://huggingface.co/MILVLG/imp-v1-3b\\n8. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: ACL (2018)\\n9. Sharma, S., El Asri, L., Schulz, H., Zumer, J.: Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. arXiv preprint arXiv:1706.09799 (2017)\\n10. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., Catanzaro, B.: Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019)\\n11. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image captioning with reading comprehension. In: ECCV (2020)\\n12. Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: CVPR (2019)\\n13. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., et al.: Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286 (2023)\\n14. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\\n15. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022)\\n16. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024)\\n17. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\\n18. Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multi-modal few-shot learning with frozen language models. NeurIPS (2021)\\n19. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image description evaluation. arXiv preprint arXiv:1411.5726 (2014)\\n20. Wang, F., Mei, J., Yuille, A.: Sclip: Rethinking self-attention for dense vision-language inference. arXiv preprint arXiv:2312.01597 (2023)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='9061feb3-50a2-461e-9eaf-8af5aa901db8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n1. Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., Jiang, Y.G.: To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574 (2023)\\n2. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)\\n3. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023)\\n4. Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021)\\n5. Welbl, J., Liu, N.F., Gardner, M.: Crowdsourcing multiple choice science questions. In: Derczynski, L., Xu, W., Ritter, A., Baldwin, T. (eds.) Proceedings of the 3rd Workshop on Noisy User-generated Text. Association for Computational Linguistics (2017)\\n6. Wenzek, G., Lachaux, M.A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin, A., Grave, E.: Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359 (2019)\\n7. Wortsman, M., Liu, P.J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J.D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J., Gilmer, J., Kornblith, S.: Small-scale proxies for large-scale transformer training instabilities (2023)\\n8. Yang, G., Hu, E.J.: Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522 (2020)\\n9. Yang, G., Hu, E.J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., Gao, J.: Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer (2022)\\n10. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 (2023)\\n11. Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J., et al.: mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499 (2023)\\n12. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023)\\n13. Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 (2023)\\n14. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. In: ICLR (2024)\\n15. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023)\\n16. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al.: Mmmu: A massive multi-discipline multi-modal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='b3f32f24-4cb6-4733-b6ef-83acc3705d5f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a machine really finish your sentence? (2019)\\n2. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang, L., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949 (2023)\\n3. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022)\\n4. Zhao, B., Wu, B., Huang, T.: Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087 (2023)\\n5. Zhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X., Wu, J., Huang, L.: Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289 (2024)\\n6. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)\\n7. Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., Tang, J.: Llava-phi: Efficient multimodal assistant with small language model. arXiv preprint arXiv:2401.02330 (2024)\\n8. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus, W.: St-moe: Designing stable and transferable sparse expert models (2022)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='4cabc1a8-db9a-4067-955e-8be79e04f2a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Appendix\\n\\n|A Dataset Details|25|\\n|---|---|\\n|A.1 Interleaved Image-Text Data|25|\\n|A.2 Text-Only Data|25|\\n|A.3 Visual Instruction Tuning Data|26|\\n|B Training Details|27|\\n|B.1 Pre-training|27|\\n|B.2 Supervised Fine-tuning (SFT)|29|\\n|C Evaluation Details|30|\\n|C.1 Pre-training Evaluation|30|\\n|C.2 SFT Evaluation Benchmarks|30|\\n|C.3 SFT Evaluation Meta-Average|30|\\n|C.4 Additional SFT Ablations|31|\\n|C.5 Implementation Details for Few-shot MM1-30B-Chat|32|\\n|D Qualitative Examples|33|\\n|E Author Contributions and Acknowledgements|40|\\n\\n# A Dataset Details\\n\\n# A.1 Interleaved Image-Text Data\\n\\nFollowing a process similar to OBELICS [58], we construct a dataset of 500M interleaved image-text documents, containing 1B images and 500B text tokens. These 500M documents are built from a collection of 3B HTML files described in Sec. A.2. From each of the HTML files, we extract the text body layer and all the &lt;img&gt; tags. We remove documents that have no images or more than 30 images. We then download the images and insert them at their original positions in the text. Finally, we perform image filtering and image de-duplication to remove low-quality and repetitive images.\\n\\nDuring image filtering, we remove images that have corrupted bytes and/or header, aspect ratio less than 1/2 or greater than 2, are too small (less than 100px) or too large (larger than 10,000px), or if their URL contains logo, button, icon, plugin or widget. During image de-duplication, we remove images whose URL or MD5 hash have appeared more than 10 times in the dataset. Additionally, when an image appears multiple times on a single page, we only retain its first appearance.\\n\\n# A.2 Text-Only Data\\n\\nFrom an initial Web corpus of 150B English HTML files, we perform boilerplate removal to arrive at the HTML representing the main content. We then follow similar processes as GPT-3 [10] and CCNet [118] to filter out documents that are too short, contain profanity, or are otherwise considered low-quality documents. We de-duplicate the data using exact-hash matching and LSH-based near-duplicate detection. Using these methods, we arrive at 3B HTML files.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='dfd1cdcd-f3af-44b3-9799-944222b190a8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Table 5: List of datasets used for supervised fine-tuning.\\n\\n|Datasets|Size|Prompting Strategy|\\n|---|---|---|\\n|Text-only SFT|13k|–|\\n|LLaVA-Conv [76]|57k| |\\n|LLaVA-Complex [76]|77k|–|\\n|ShareGPT-4V [15]|102k| |\\n|VQAv2 [38]|83k| |\\n|GQA [46]|72k| |\\n|OKVQA [82]|9k| |\\n|OCRVQA [86]|80k|“Answer the question using a single word or phrase.”|\\n|DVQA [51]|200k| |\\n|ChartQA [83]|18k| |\\n|AI2D [52]|3k| |\\n|DocVQA [85]|39k| |\\n|InfoVQA [84]|24k| |\\n|A-OKVQA [98]|66k|“Answer with the option’s letter from the given choices directly.”|\\n|COCO Captions [18]|83k|Sample from a pre-generated prompt list, e.g., “Provide a brief description of the given image.”|\\n|TextCaps [103]|22k|Sample from a pre-generated prompt list, e.g., “Please transcribe all the text in the picture.”|\\n|SynthDog-EN [53]|500k| |\\n|Total|1.45M|–|\\n\\n# A.3 Visual Instruction Tuning Data\\n\\nOur final SFT data mixture contains a variety of datasets, mostly follow LLaVA-1.5 [74] and LLaVA-NeXT [75]. Specifically,\\n\\n- To encourage the model to provide long-form detailed responses and perform conversations, we follow previous work, use the existing GPT-4 generated data (LLaVA-Conv and LLaVA-Complex [76]) and the existing GPT-4V generated data (ShareGPT-4V [15]) for model training. We also experimented with LAION-GPT4V, but did not observe further performance improvement, thus not included in the final mixture.\\n- To enhance the model with better vision-language (VL) understanding capability, we use a variety of academic task oriented VL datasets. These datasets are either in the form of image captioning, or in the form of VQA with short answers. Specifically,\\n- For natural images: VQAv2 [38], GQA [46], OKVQA [82], A-OKVQA [98], and COCO Captions [18];\\n- For text-rich images: OCRVQA [86], and TextCaps [103];\\n- For document and chart understanding: DVQA [51], ChartQA [83], AI2D [52], DocVQA [85], InfoVQA [84], and SynthDog-En [53];\\n- To enhance the model’s text-only instruction following capability, we also blend in a small amount of text-only SFT data.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c6778cb1-49fe-4706-887d-7b6d9564aadc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# 27\\n\\nThe academic task oriented image captioning and VQA datasets are formatted into the instruction-following format, following LLaVA-1.5 [74], with detailed prompts summarized in Table 5.\\n\\n# B Training Details\\n\\n# B.1 Pre-training\\n\\nBatch Size and Composition. For simplicity, all MM1 models are pre-trained with the same batch size of 512 and maximum decoder sequence length of 4096. We allow up to 16 images per input sequence, with each image resulting in 144 tokens as input to the decoder. Note that this results in roughly 1M text tokens and 1M image tokens per batch. Each input sequence is sampled from one of three types of input sources: (1) interleaved, (2) packed image-text pairs, or (3) text-only data, with sampling probability 45%, 45%, and 10%, respectively. When packing image-text pairs or interleaved documents along the sequence dimension, we modify the self-attention masks to prevent tokens from attention across example boundaries. For image-text pairs in particular, this was critical for maintaining strong few-shot performance.\\n\\nWeight Decay. Note that our sampling/mixing procedure is performed once offline and stored as a fixed deterministic snapshot of our pre-training mixture. This means, with the exception of our ablations on the pre-training mixture itself, all models in this paper are trained on the same examples in the same order. We found this was critical to ensure internal reproducibility of our results, as initial experiments showed that different random seeds in the input pipeline could have non-negligible impact on resulting models.\\n\\n**Table 7: Predicted optimal peak learning rate η and weight decay λ for MM1 model sizes.**\\n|N|Pred. η|Pred. λ|\\n|---|---|---|\\n|1.2B|8.6e-5|5.0e-6|\\n|2.9B|5.9e-5|3.5e-6|\\n|6.4B|4.2e-5|2.5e-6|\\n|30B|2.2e-5|1.3e-6|\\n\\nLearning Rate Schedule. For multimodal pre-training, MM1 employs a standard cosine learning rate decay schedule with an initial linear warmup of 2000 steps. The learning rate is then decayed to 10% of its peak value over the course of 2e5 training steps. We perform gradient clipping with max norm 1 and use the AdamW optimizer with an implementation that decouples the learning rate and weight decay. For MM1-30B, we also add a z-loss term with scale 1e-4, as we observed this improves training stability, similar to [119].\\n\\n**Fig. 8: Optimal weight decay as a function of model size for the grid searches described in Sec. B.1.**\\n|10−4| | | | | |10−5| | | | |10−6|\\n|---|---|---|---|---|---|---|---|---|---|---|---|\\n|10M|100M|1.0B|10B| | | | | | | |Num LLM Params|', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='09efa55c-db5f-48f8-8251-64aaa61335a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 28\\n\\n# B. McKinzie et al.\\n\\n|Model|Shot| |Captioning| | |Visual Question Answering| | |\\n|---|---|---|---|---|---|---|---|---|\\n| | |COCO|NoCaps|TextCaps|VQAv2|TextVQA|VizWiz|OKVQA|\\n|MM1-3B Model Comparisons|0†|73.0|–|–|49.2|30.1|28.9|41.2|\\n|Flamingo-3B [3]|4|85.0|–|–|53.2|32.7|34.0|43.3|\\n| |8|90.6|–|–|55.4|32.4|38.4|44.6|\\n| |16|95.3|–|–|56.7|31.8|43.3|45.6|\\n| |0|73.5|55.6|63.3|46.2|29.4|15.6|26.1|\\n|MM1-3B|4|112.3|99.7|84.1|57.9|45.3|38.0|48.6|\\n| |8|114.6|104.7|88.8|63.6|44.6|46.4|48.4|\\n| |16|116.8|107.6|91.6|60.9|46.1|53.8|50.5|\\n|MM1-7B Model Comparisons|0†|46.0*|36.8|25.4|50.9|25.9|35.5|38.4|\\n|IDEFICS-9B [58]|4|93.0*|81.3|60.0|55.4|27.6|36.9|45.4|\\n| |8|97.0*|86.8|63.2|56.4|27.5|40.4|47.7|\\n| |16|99.7*|89.4|67.4|57.0|27.9|42.6|48.4|\\n| |0†|79.4|–|–|51.8|31.8|28.8|44.7|\\n|Flamingo-9B [3]|4|93.1|–|–|56.3|33.6|34.9|49.3|\\n| |8|99.0|–|–|58.0|33.6|39.4|50.0|\\n| |16|102.2|–|–|59.4|33.5|43.0|50.8|\\n| |0†|–|–|–|52.9|–|34.4|42.8|\\n|Emu2-14B [105]|4|–|–|–|58.4|–|41.3|–|\\n| |8|–|–|–|59.0|–|43.9|–|\\n| |0|76.3|61.0|64.2|47.8|28.8|15.6|22.6|\\n|MM1-7B|4|109.8|96.2|84.5|60.6|44.4|37.4|46.6|\\n| |8|116.3|106.6|88.2|63.6|46.3|45.3|51.4|\\n| |16|118.6|111.1|93.1|65.2|46.9|53.2|52.9|\\n|MM1-30B Model Comparisons|0†|91.8*|65.0|56.8|60.0|30.9|36.0|45.2|\\n|IDEFICS-80B [58]|4|110.3*|99.6|72.7|63.6|34.4|40.4|52.4|\\n| |8|114.3*|105.7|77.6|64.8|35.7|46.1|55.1|\\n| |16|116.6*|107.0|81.4|65.4|36.3|48.3|56.8|\\n| |0†|84.3|–|–|56.3|35.0|31.6|50.6|\\n|Flamingo-80B [3]|4|103.2|–|–|63.1|36.5|39.6|57.4|\\n| |8|108.8|–|–|65.6|37.3|44.8|57.5|\\n| |16|110.5|–|–|66.8|37.6|48.4|57.8|\\n| |0|–|–|–|33.3|26.2|40.4|26.7|\\n|Emu2-37B [105]|4|–|–|–|67.0|48.2|54.6|53.2|\\n| |8|–|–|–|67.8|49.3|54.7|54.1|\\n| |16|–|–|–|68.8|50.3|57.0|57.1|\\n| |0|70.3|54.6|64.9|48.9|28.2|14.5|24.1|\\n|MM1-30B|4|117.9|103.8|87.5|68.8|48.1|41.7|54.9|\\n| |8|123.1|111.6|92.9|70.9|49.4|49.9|58.3|\\n| |16|125.3|116.0|97.6|71.9|50.6|57.9|59.3|\\n\\nTable 6: Complete MM1 pre-training few-shot evaluation results. (*) IDEFICS includes PMD in its training data (includes COCO). (†) These models included two text-only demonstrations in their “0” prompt, whereas MM1 does not.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2df33cb3-48d8-4138-939c-aac66554cfb2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# 2.2E-05\\n\\n|9m|85m|\\n|---|---|\\n|20|40|\\n|2.2E-05|2.2E-05|\\n|4.6E-05|4.6E-05|\\n|15|30|\\n|1.0E-04|1.0E-04|\\n|2.2E-04|2.2E-04|\\n|4.6E-04|4.6E-04|\\n|1.0E-03|1.0E-03|\\n|5|1.0E-03|\\n|103.2E-06|3.2E-06|\\n|2.2E-03|2.2E-03|\\n|1.0E-05|1.0E-05|\\n|3.2E-05|3.2E-05|\\n|(a) 9M|(b) 85M|\\n|302m|1.2b|\\n|601.0E-04|1.0E-04|\\n|2.2E-05|2.2E-05|\\n|50|4.6E-05|\\n|40|50|\\n|1.0E-04|1.0E-04|\\n|2.2E-04|2.2E-04|\\n|30|2.2E-04|\\n|40|4.6E-04|\\n|1.0E-03|1.0E-03|\\n|20|1.0E-03|\\n|30|2.2E-03|\\n|10|3.2E-06|\\n|1.0E-05|1.0E-05|\\n|(c) 302M|(d) 1.2B|\\n\\nFig. 9: 8-shot average for grid searches over peak learning rate (y-axis) and weight decay (x-axis) for different LLM sizes. Black cells correspond to settings we did not run a corresponding experiment for.\\n\\nIn this work are shown in Table 7. For simplicity, for the actual MM1 3B, 7B, and 30B models, we used η equal to 6e-5, 4e-5, and 2e-5, respectively. Finally, we fix the peak LR of the randomly initialized vision-language connector of MM1 to η =8e-5 for all model sizes. For future versions of MM1, we plan on incorporating techniques similar to [121] to avoid the need to conduct costly hyperparameter searches.\\n\\n# B.2 Supervised Fine-tuning (SFT)\\n\\nThe model is fine-tuned for 10k steps with batch size 256 and sequence length 2048. We employ the AdaFactor optimizer with peak learning rate 1e-5 and', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='a0a33cd5-61da-48c9-b895-1744e13ee240', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# C      Evaluation Details\\n\\n# C.1     Pre-training Evaluation\\n\\nFew-shot prompts are randomly sampled per-dataset from the training set if available, otherwise the validation set (ensuring the query example does not appear in any of the shots). Outputs are generated with greedy decoding until the model emits the EOS token or any additional stop tokens that can be specified on a per-task basis. The additional stop token for captioning tasks is just the newline character, and for VQA tasks we also include “.”, “,”, and “Question” as valid stop tokens. For postprocessing VQA predictions, we use the same logic as OpenFlamingo10 [4]. For captioning tasks, we report CIDEr score [111] using the nlg-eval package [101]. All of our multimodal pre-training evaluations are implemented in an internal fork of EleutherAI’s lm-evaluation-harness [35].\\n\\n|Dataset|Evaluation Split|\\n|---|---|\\n|COCO|Karpathy test|\\n|NoCaps|val|\\n|TextCaps|val|\\n|VQAv2|testdev|\\n|TextVQA|val|\\n|VizWiz|testdev|\\n|OKVQA|val|\\n\\nTable 8: Splits used for pre-training evaluation. Note that, unlike the main pre-training results, all pre-training ablations use the validation splits for VQAv2 and VizWiz.\\n\\n# C.2     SFT Evaluation Benchmarks\\n\\nWe evaluate our SFT models on a collection of both traditional academic VL benchmarks and recent benchmarks specifically designed for MLLMs. For academic VL benchmarks, we include VQAv2 [38], TextVQA [104], and the image subset of ScienceQA [81]. For recent MLLM benchmarks, we include POPE [68], MME [33], MMBench [78], SEED-Bench [62], LLaVA-Bench-in-the-Wild [76], MM-Vet [127], MathVista [80], and the recent popular MMMU [128]. For all the benchmarks, we use greedy decoding to generate the responses. For MM-Vet and LLaVA-Bench-in-the-Wild, which use GPT-4 for evaluation, we run the evaluation 3 times, and report the average.\\n\\n# C.3     SFT Evaluation Meta-Average\\n\\nIn the process of SFT ablation, we synthesize all benchmark results into a single meta-average number to simplify comparisons. Because the evaluation metrics of different datasets may have different ranges, we normalize with respect to a baseline configuration. This is achieved by initially standardizing the results for 10 Specifcally, the implementation of VQAMetric (commit 60a5fd6).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2e2955b7-65d3-4624-81d3-5ea145224f62', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Average Performance\\n\\n| |Unfrozen Encoder|Frozen Encoder|\\n|---|---|---|\\n|Avg. Pool|100|99.6|\\n|Att. Pool|97.4|97.3|\\n|50/50/0|50/0/50|100/0/0|\\n|42/42/14|45/45/9| |\\n\\n# Figure 10: SFT ablations\\n\\n(a) The impact of pre-training data mixture on SFT results. Here, x/y/z means that x% of the data is interleaved, y% is captions, and z% is pure text. tks: the number of image tokens. (b) The impact of different vision-language connectors on SFT results. For both (a) and (b), we first pre-train MM1-3B with the ablated setting, and then perform SFT on the pre-trained models. (c) Freezing or unfreezing the image encoder during SFT.\\n\\n# C.4 Additional SFT Ablations\\n\\nIn this section, we perform SFT ablations. This section is analogous to Section 3; here, we perform SFT on the same checkpoints and evaluate if similar lessons hold true on SFT evaluations, instead of pre-training evaluations. Furthermore, we also study whether to keep the image encoder frozen or not during SFT. For all of these ablations, we train MM1-3B-Chat.\\n\\n# Pre-training data mixture ablations\\n\\nIn Figure 10a, we compare the SFT performance with different weights for pre-training data. We see a similar trend when comparing with Figure 5 for 0-shot evaluations. Pre-training with caption-only data gives the best performance across the SFT evaluation metrics. This corroborates Data lesson 1: caption data still lifts zero-shot performance for SFT evaluations. However, the SFT metrics do not measure few-shot performance, so the impact of the interleaved data is not noticeable in this table.\\n\\n# Visual-language connector ablations\\n\\nIn Figure 10b, we evaluate different visual-language connector configurations. This figure is similar to Figure 4, except that we evaluate the corresponding SFT models. As can be seen, if a low number of image tokens is used, average pooling gives similar results as C-Abstractor. When the number of image tokens is increased, the C-Abstractor configuration gives the best results. These trends are not entirely consistent with pre-training results reported in Figure 4. Overall, the impact of the choice.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='b084dc37-37e8-478e-9b00-c371d5f35053', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# C.5 Implementation Details for Few-shot MM1-30B-Chat\\n\\nAs shown in Section 5.1, our fine-tuned model can utilize in-context examples to achieve even stronger performance. Interestingly, the performance goes up when increasing the number of examples. We demonstrate this with MM1-30B-Chat.\\n\\nOne challenge for few-shot inputs arises due to the use of sub-image decomposition. While this strategy lifts zero-shot performance, it significantly increases the effective number of tokens consumed per image. Using 5 sub-images per input image as MM1-30B-Chat does, processing a 4-shot example where every example contains just one source image already yields 20 effective images. Representing every image with 144 tokens therefore requires 2,880 tokens for images alone, quickly exhausting limited language model context. To mitigate this limitation, we propose a new mixed-resolution approach. Specifically, for K in-context examples, we only encode the last N images at a high resolution via sub-image decomposition, the remaining K − N in-context examples are processed at lower resolution. This makes in-context examples much less expensive to encode and allows to increase the number of in-context examples within a fixed token budget, further increasing performance. In our implementation, we set N = 3.\\n\\nWe demonstrate the effectiveness of our proposed strategy using the Math-Vista benchmark. Using in-context learning with chain-of-thought, the performance of MM1-30B-Chat improves from 39.4 (0-shot) to 41.9 (4-shot, with all in-context examples using full sub-image decomposition). Applying our mixed-resolution approach allows to encode additional in-context examples, enabling up to 8-shot chain of thought, further improving the performance to 44.4. This illustrates that our MM1-Chat model retains in-context learning capabilities inherited from its pre-training regimen, and that our strategy of mixed decomposition is effective at further increasing few-shot performance within a restricted context length.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2a9ff9ba-61f1-4b41-a074-5b27d271f686', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Qualitative Examples\\n\\nIn this section, we share qualitative examples of MM1 predictions.\\n\\n|User: Count the number of apples in the image.|MM1-30B-Chat (Ours): There are seven apples in the image.|\\n|---|---|\\n|User: Count the number of oranges in the image.|MM1-30B-Chat (Ours): There are three oranges in the image.|\\n|User: What are all the scene text in the image?|MM1-30B-Chat (Ours): The earth without art is just 'eh'|\\n|User: Which of these states is farthest north?|MM1-30B-Chat (Ours): Delaware|\\n|User: What are the other states?|MM1-30B-Chat (Ours): Arizona, Oklahoma, North Carolina|\\n|User: Which of the organisms is the producer in this food web?|MM1-30B-Chat (Ours): berries and flowers, grasses, seeds|\\n\\nFig. 11: Examples testing MM1 counting, OCR and scientific knowledge capabilities. Images and prompts are from COCO 2014 validation set [72] and [122].\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0538cfb3-2782-45ba-ab75-c8c1a75bfa2a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# AFM-Multimodal v3: finetune-29_6b-huge dfn-378px-cabstractor-144-672px-sphinx (newO) instructed (Greedy Decoding)\\n\\n# 34 B. McKinzie et al.\\n\\nUser: Can you find the red ball near the gray cat in the image?\\n\\nMM1-30B-Chat (Ours): No, there is no red ball near the gray cat in the image. The cat is sitting on a pillow on a couch, and there are no other objects like a red ball visible in the scene.\\n\\nUser: From scale 1 to 10, decide how similar the image is to the text prompt \"a parrot driving a car\". Explain your reasoning.\\n\\nUser: Can you see the bright blue feathers on the bird\\'s wing?\\n\\nMM1-30B-Chat (Ours): No, the bird in the image does not have bright blue feathers on its wing. The bird is a large bird of prey, such as a hawk or an eagle, with typical bird of prey feathers.\\n\\nUser: From scale 1 to 10, decide how similar the image is to the text prompt \"a parrot driving a car\". Explain your reasoning.\\n\\nMM1-30B-Chat (Ours): I would rate the image a 1 on the scale of 1 to 10 in relation to the text prompt \"a parrot driving a car.\" The image shows a dolphin jumping out of the water, and the text prompt is referring to a parrot driving a car. There is no similarity between the image and the text prompt, other than the fact that both contain animals. The dolphin is in the water, while the parrot is associated with a car. The actions of the animals are also different, as the dolphin is jumping out of the water, while the parrot is implied to be driving the car. The other parrot is comfortably seated on the roof of the car, seemingly enjoying the view. The colors in the image are vibrant, with the blue of the car contrasting beautifully with the green field and the yellow sky. The palm trees add a tropical feel to the scene.\\n\\nThe image is a close representation of the text prompt, with the exception of the number of parrots and their actions. While the prompt mentions only one parrot driving a car, the image shows two parrots, one seemingly driving and the other on the roof of the car. This adds an element of surprise and humor to the image. Overall, the image scores a 9 out of 10 for its close representation of the text prompt and its creative interpretation.\\n\\nFig. 12: Examples testing MM1 against adversarial prompts and image-prompt alignment. Images and prompts are from COCO 2014 validation set [72], [122] and [8].', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c34dc5e0-7abd-4a78-a765-9b40e6650296', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# User: After seeing the below image, how people might emotionally feel and react?\\n\\nMM1-30B-Chat (Ours): Upon witnessing this image, people might feel a mixture of emotions such as awe, fascination, and tranquility. The breathtaking view of the airplane window showing the wing of the plane and a vast, snowy mountain range beneath it offers a sense of serenity and the beauty of nature. The magnificent, orderly pattern of the mountains and their snowy peaks contrasted with the clear blue sky evokes a sense of wonder and appreciation for the natural world. This view also offers a unique perspective of the world that one might not experience daily, thus sparking curiosity and excitement. Additionally, the image can bring about feelings of calmness and peace, given the untouched and pristine nature of the landscape depicted.\\n\\n# User: After seeing the below image, how people might emotionally feel and react?\\n\\nMM1-30B-Chat (Ours): The image shows a dark stairwell with a creepy atmosphere, which might evoke feelings of unease, fear, or curiosity in people. Some individuals may react by avoiding the stairwell altogether, while others might be intrigued by the mysterious ambiance and decide to explore it further. The darkness and the shadows in the stairwell can play with people's perceptions and trigger their imagination, possibly making them feel as if there is someone or something lurking in the shadows. Overall, the image can provoke a range of emotional responses, from apprehension to fascination, depending on each person's disposition and mindset.\\n\\n# User: <img1> <img2> For each of the two images comment on whether the water is salty.\\n\\nMM1-30B-Chat (Ours): In the first image, there are several boats floating on a body of water. Based on the presence of boats, it's likely this is a harbor, and the water is likely salty as most harbors are typically found near the ocean. However, without more information, it's not possible to confirm the salinity of the water.\\n\\nIn the second image, various African animals, including elephants and giraffes, are gathered around a watering hole in a dry grass field. This body of water is most likely freshwater as animals need fresh water to drink. The presence of numerous animals drinking from the watering hole confirms that this water is essential for their survival, and it's highly unlikely they would be able to drink saltwater.\\n\\n# User: <img1> <img2> Which image shows the healthier meal?\\n\\nMM1-30B-Chat (Ours): The healthier meal among the given images is the one that features broccoli and a bowl of vegetables, along with a plate of meat. Incorporating vegetables, like broccoli and those in the bowl, provides essential nutrients, vitamins, and fiber, which contribute to a balanced and healthier diet. The meal with various toppings on a pizza and beer would be considered less healthy as it may contain more calories, fat, and processed ingredients compared to the other meal that emphasizes vegetables and meat.\\n\\nFig. 13: Examples testing MM1 ability to perceive image aesthetics and compare multiple images. Images and prompts are from COCO 2014 validation set [72] and [122].\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0d9164bf-4aec-4fe1-9b65-c969ccfaf8b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# AFM-Multimodal v3: finetune-29_6b-huge_dfn-378px-cabstractor-144-672px-sphinx (newO) instructed (Greedy Decoding)\\n\\nread the operating menus below for coffee machine buttons\\n\\n6 Oz\\n8 Oz\\n10 Oz\\nNow, let\\'s look at the operating panel of the coffee machine in the image below, which button should I use for a 6 OZ coffee: left, right, top or bottom button?\\n\\nUser: Describe the image\\n\\nMM1-30B-Chat (Ours): The image captures a moment from the perspective of a car\\'s dashboard camera. The car is traveling on a two-lane road, divided by a yellow line, suggesting a rural or less populated area. The road is flanked by trees on both sides, their green leaves contrasting with the gray, cloudy sky above. A speed limit sign displaying \"35\" is visible on the right side of the road, indicating the maximum speed allowed for vehicles. The overall scene suggests a calm and serene drive through a lightly wooded area.\\n\\nTo brew a 6 OZ coffee using the coffee machine in the image, you should use the left button. This is because the buttons on the machine correspond to the menu icons, and the left button corresponds to the 6 OZ menu icon. The other buttons correspond to different cup sizes, but the left button is the one you need to press to brew a 6 OZ coffee.\\n\\nFigure: Following [Yang 2023], we use MM1 on task-oriented scenarios such as operating machines and navigating. Images are from [Yang 2023].\\n\\nFig. 14: Following [122], we tested MM1 on task-oriented scenarios such as operating machines and navigating. Images and prompts are from [122].', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2675793b-f962-47b2-bc08-31bf9e3b2b74', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# AFM-Multimodal v3: finetune-29_6b-huge dfn-378px-cabstractor-144-672px-sphinx (newO) instructed (Greedy Decoding)\\n\\n# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nUser: Suppose you are a teacher, please use the figure to explain the distinction between evaporation and evapotranspiration.\\n\\nMM1-30B-Chat (Ours): Evaporation and evapotranspiration are two important processes in the water cycle that are often confused. The image provides a clear distinction between the two. Evaporation is the process where water changes from a liquid state to a gaseous state. This is represented in the image by the sun\\'s heat causing water in the lake and ocean to turn into water vapor and rise into the atmosphere.\\n\\nUser: Explain the figure.\\n\\nMM1-30B-Chat (Ours): The image presents a flowchart that outlines the process of how a team member suggests a project. The flowchart begins with a green rectangle that contains the text \"A team member suggests a project\". From this starting point, the process flows through a series of diamond-shaped decision points and rectangular action points, each containing specific text that describes the next step in the process.\\n\\nThe process continues with the team discussing the idea in a meeting, followed by a decision point where participants decide if it\\'s a good idea. If it\\'s a good idea, the process moves forward. If not, the process loops back to the point where the team member conducts more research.\\n\\nIn summary, evaporation is the process of water turning into vapor and rising into the atmosphere, while evapotranspiration is the combination of evaporation and plant transpiration, representing the water absorbed by plants and released back into the atmosphere.\\n\\nThe flowchart is color-coded with green rectangles representing actions and yellow diamonds representing decisions. The lines connecting these shapes are black, indicating the sequence of the process. The flowchart is set against a white background, which makes the colored shapes stand out clearly.\\n\\nThe flowchart provides a clear and structured representation of the process, making it easy to understand the sequence of steps involved in the project suggestion and decision-making process. It\\'s a useful tool for visualizing complex processes and ensuring that all steps are considered and followed in the correct order.\\n\\n# Figure:\\n\\nMM1 is able to accurately extract information from graphics. Prompts and images from Figure 31 & 35 in [Yang 2023].\\n\\nFig. 15: Examples testing MM1 ability at extracting information from graphics. The right part shows an example of confusion, highlighted in red. Images and prompts are from [122].', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='610c0c7f-7075-4263-beae-b762b887dc9c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# AFM-Multimodal v3: finetune-29_6b-huge_dfn-378px-cabstractor-144-672px-sphinx (newO) instructed (Greedy Decoding)\\n\\nFrom scale 1 to 10, decide how similar the image is to the text prompt \"3 parrots driving a car\". Explain your reasoning:\\n\\nThe image shows a dolphin jumping out of the water, and the text prompt is referring to 3 parrots driving a car. There is no similarity between the image and the text prompt, other than the fact that both contain animals. The dolphin is in the water, while the parrot is associated with driving. The actions of the animals are also different, as the dolphin is jumping out of the water, while the parrot is implied to be driving. The dolphin is in a natural aquatic environment and the parrot is in a human-made terrestrial environment.\\n\\nThe image portrays two parrots situated inside a blue car. The parrot in the driver’s seat appears to be in control of the vehicle, giving the impression that it is driving the car. The other parrot is seated in the passenger seat. The car is set against a yellow sky, adding a vibrant touch to the overall scene.\\n\\nThe image is colorful and lively, capturing the essence of the text prompt quite accurately. The only difference is that there are two parrots instead of one driving the car. Overall, the beautiful backdrop with its clear blue hue and scattered white clouds enhances the scene. The image does not contain any text or other discernible objects. The toucan\\'s position and the direction of its flight suggest it is exploring the vast sky above the treetops. The image does not provide any indication of the bird driving a car, as per the text prompt. The similarity rating between the image and the text prompt \"parrot driving a car\" would be 0 out of 10, as the image does not depict any such scene.\\n\\n# Upload Image\\n\\nFrom scale 1 to 10, decide how similar the image is to the text prompt \"parrot driving a car\". Explain your reasoning:\\n\\nThe image portrays two parrots situated inside a blue car. The car is parked on a grassy field with a backdrop of palm trees and a blue sky. The first parrot, which is perched on the steering wheel, gives the impression that it is driving the car. The image is colorful and vibrant, with the bright colors of the parrots contrasting against the blue car and the green field. The palm trees and blue sky contribute to the overall tropical feel of the scene.\\n\\nThe image accurately depicts a parrot or two driving a car. The only difference is that there are two parrots instead of one, and they are situated in a parked car rather than actively driving on a road. The overall theme of the image is a playful and imaginative representation of the text prompt. Therefore, I would rate the image as 8 out of 10 for its similarity to the text prompt. It successfully captures the main elements of the prompt while adding its own unique touches to create a visually appealing and engaging scene.\\n\\n# User: Which book has been written by Heller?\\n\\nMM1-30B-Chat (Ours): Catch-22\\n\\n# User: And who wrote \"Of Mice & Men\"?\\n\\nMM1-30B-Chat (Ours): John Steinbeck\\n\\n# User: I will show you several pictures and your task is to answer my question.\\n\\nThe image portrays a scene that is quite similar to the text prompt \"parrot driving a car\". In the image, there are two parrots situated inside a blue car. The car is parked on a green field with a backdrop of palm trees, with one parrot perched on the steering wheel, giving the impression that it is driving the car. The other parrot is comfortably seated on the roof of the car, seemingly enjoying the view. The colors in the image are vibrant, with the blue of the car contrasting beautifully with the green field and the yellow sky. The palm trees add a tropical feel to the scene.\\n\\nThe image is a close representation of the text prompt, with the exception of the number of parrots and their actions. While the prompt mentions only one parrot driving a car, the image shows two parrots seemingly driving and the other on the roof of the car. This adds an element of surprise and humor to the image. Overall, the image scores a 9 out of 10 for its close representation of the text prompt and its creative interpretation.\\n\\n# Estimate the temperature in Celsius for each image. Also provide reasons.\\n\\nMM1-30B-Chat (Ours):\\n\\n- The image with the umbrella on the beach: 25-30 degrees Celsius. The umbrella is up, and people are at the beach, suggesting a warm, sunny day.\\n- The image with the plane flying over the snow: -20 to -30 degrees Celsius. The plane is flying over a snow-covered area, and there are vehicles on the ground, indicating a cold winter day.\\n- The image with the animals grazing in the field: 15-20 degrees Celsius. The animals are grazing, which usually occurs in milder weather conditions. The grass is green, further suggesting a temperate climate.\\n\\nFig. 16: Examples testing MM1 ability at reasoning across images and texts. Images are from COCO 2014 validation set [72] and MMMU dev set [128].\\n\\nFigure: Images from MMMU', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2da76b4e-a1ad-4b16-b84f-5cbcdc33effa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nUser:\\n\\nImage A: 013\\n\\nFor each image state what's the number shown on the cake. Then return the sum.\\n\\nMM1-30B-Chat (Ours): The first image shows a cake with the number 3 on it, and the second image shows a cake with the number 4 on it. The sum is 7.\\n\\nUser: What is the color of the balloon?\\n\\nUser: I am showing you an image and you need to predict the letter or number shown when rotating the image by 180 degrees.\\n\\nThe color is ! R_E_D !✨\\n\\nMM1-30B-Chat (Ours): 9\\n\\nThe color is # G_R_E_E_N #✨\\n\\nMM1-30B-Chat (Ours): The color is $ B_L_U_E $✨\\n\\nFigure: Images from COCO Val set\\n\\nFig. 17: Examples testing MM1 ability to follow instructions across multiple images (top). Examples testing MM1 at following a style or a task across few shots (bottom). Images are from COCO 2014 validation set [72] and from the authors.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='426dd1b5-c944-4d8e-8db4-d819170770d9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# E. Author Contributions and Acknowledgements\\n\\n# First Authors\\n\\nBrandon McKinzie: Led the multimodal pre-training design, implementation, and experimentation. Co-led design and implementation of multimodal evaluation infrastructure. Co-set overall direction. Co-wrote paper.\\n\\nZhe Gan: Led the SFT design, implementation, and experimentation. Co-initiated effort. Co-set overall direction. Co-wrote paper.\\n\\n# Core Authors\\n\\nJean-Philippe Fauconnier: Co-led design and implementation of multimodal evaluation infrastructure, assisted with model evaluations, model implementation, multimodal pre-training and SFT experimentation.\\n\\nSam Dodge: Assisted with SFT experimentation, data mixtures, and multimodal evaluation infrastructure.\\n\\nBowen Zhang: Co-initiated effort, trained image encoders, assisted with infrastructure.\\n\\nPhilipp Dufter: Assisted with model implementation, evaluations, and experimentation.\\n\\nDhruti Shah: Implemented interleaved SFT, assisted with experimentation.\\n\\nXianzhi Du: Implemented and trained MoE for multimodal pre-training, SFT and underlying LLM.\\n\\nPeter Grasch: Advised and analyzed experiments, co-led design and implementation of multimodal evaluation infrastructure, co-wrote paper.\\n\\n# Further Authors\\n\\nFutang Peng: Data processing and coordination.\\n\\nFloris Weers: Led text-based evaluation infrastructure and assisted with multimodal evaluation infrastructure.\\n\\nHaotian Zhang: Implemented and experimented with MoE models.\\n\\nAnton Belyi, Karanjeet Singh, Doug Kang, Ankur Jain: Dataset creation and filtering.\\n\\nHongyu Hè: Co-implemented VL connector, assisted with experimentation.\\n\\nMax Schwarzer: Implemented support for pre-training on packed image-text pairs and packed interleaved documents.\\n\\nTom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee: Designed, implemented, and trained the underlying LLMs.\\n\\nZirui Wang, Ruoming Pang: Co-initiated effort, designed, implemented, and trained the underlying LLMs.\\n\\n# Senior Authors\\n\\nAlexander Toshev: Co-set overall direction, advised and analyzed experiments, co-wrote paper.\\n\\nYinfei Yang: Co-initiated effort, co-set overall direction, advised and analyzed experiments, co-wrote paper.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='84852e76-5e07-4026-a674-34cb14b9a2ce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\n# Acknowledgements\\n\\nThe authors would like to thank Vaishaal Shankar, Alaa El-Nouby, Yang Zhao, Shuangfei Zhai, Russ Webb, Hadi Pouransari, Hong-You Chen, Yanghao Li, and David Mizrahi for valuable guidance, suggestions, and feedback; Chen Chen and Qibin Chen for help on instruction tuning; Maitreyi Kunnavakkam Vinjimur, Megan Maher Welsh, Bhavika Devnani, and David Koski for their assistance with input pipelines and data processing; Tom Nickson and Michael Tu for assistance with the AXLearn infrastructure and early LLM work; Varsha Mohan Paidi for assistance with dataset creation and filtering; Esteban Gonzalez, Ian Clark, Jack Bailin, David Koski, and in particular Venkata Yerneni for assistance with the internal Weights & Biases instance for tracking experiments and model evaluations.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"), num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "1it [00:00, 996.75it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "3it [00:00, 1506.03it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "2it [00:00, 1998.24it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The table compares the estimated total price of beers on a table according to different models. MM1-30B-Chat estimates the price as 12, Emu-Chat-37B states the price as 15.99, and LLaVA-NeXT-34B provides a detailed calculation estimating the price as 44 based on assumed quantities and types of beer.,\\nwith the following columns:\\n- MM1-30B-Chat (Ours): None\\n- Emu-Chat-37B: None\\n- LLaVA-NeXT-34B: None\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\\n\\nBrandon McKinzie◦, Zhe Gan◦, Jean-Philippe Fauconnier⋆, Sam Dodge⋆, Bowen Zhang⋆, Philipp Dufter⋆, Dhruti Shah⋆, Xianzhi Du⋆, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch⋆, Alexander Toshev†, and Yinfei Yang†\\n\\nApple\\n\\n◦First authors; ⋆Core authors; †Senior authors\\n\\nbmckinzie@apple.com, zhe.gan@apple.com\\n\\n Abstract\\n\\nIn this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published multimodal pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE) variants up to 64B, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.\\n\\n 1 Introduction\\n\\nIn recent years, the research community has achieved impressive progress in language modeling and image understanding. Thanks to the availability of large-scale image-text data and compute at scale, we have seen the emergence of highly performant Large Language Models (LLMs) [9, 10, 19, 21, 26, 92, 93, 102, 107, 109, 116, 131] and Vision Foundation Models [40, 88, 91] that have become the de-facto standard for the majority of language and image understanding problems.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump both indexed tables and page text into the vector index\n",
    "recursive_index = VectorStoreIndex(nodes=base_nodes + objects + page_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 3.3 Pre-training Data Ablation\n",
      "\n",
      "Large-scale and task-appropriate data is of paramount importance in training performant models. Typically, models are trained in two stages, pre-training and instruction tuning. In the former stage web-scale data is used while in the latter stage task-specific curated data is utilized. In the following, we focus on the pre-training stage and elaborate our data choices (see Figure 3, right).\n",
      "\n",
      "|Data Type|Sources|Size|\n",
      "|---|---|---|\n",
      "|Captioned Images|CC3M [100], CC12M [13], HQIPT-204M [94], COYO [11], Web Image-Text-1B (Internal)|2B image-text pairs|\n",
      "|Captioned Images (Synthetic)|VeCap [57]|300M image-text pairs|\n",
      "|Interleaved Image-Text|OBELICS [58], Web Interleaved (Internal)|600M documents|\n",
      "|Text-only|Webpages, Code, Social media, Books, Encyclopedic, Math|2T tokens|\n",
      "\n",
      "Table 2: List of datasets for pre-training multimodal large language models.\n",
      "\n",
      "Two types of data are commonly used to train MLLMs: captioning data consisting of images with paired text descriptions; and interleaved image-text documents from the web (see Appendix A.1 for details). Note that captioning data tends to contain relatively short text with high relevance to the image. On the contrary, interleaved data has substantially longer and more diverse text with less relevance, on average, to the surrounding images. Finally, we include text-only data to help preserve the language understanding capabilities of the underlying pre-trained LLM. The full list of datasets is summarized in Table 2.\n",
      "\n",
      "We use the same model setup for ablations described in Section 3.1, with the only exception that we train 200k steps here to fully leverage the large-scale data training. We also incorporate a set of commonly employed text tasks, referred to as TextCore1, as part of the evaluation to better assess the effects of data mixture. These lead to the following lessons:\n",
      "\n",
      "Data Lesson 1: Interleaved data is instrumental for few-shot and text-only performance, while captioning data lifts zero-shot performance. In Figure 5a, we present results across different mixes of interleaved and captioned data. Zero-shot performance increases consistently, from 25.8% to 39.3%, as we increase the amount of captioned data. At the same time, however, for 4- and 8-shot performance, having at least 50% of the data being interleaved is crucial to maintain over 61% for 8-shot or 58% for 4-shot. Without it, performance drops drastically to 45% and 43.7%, respectively. Since interleaved data naturally contains multiple images and accompanying text which are often interrelated, such data is inherently similar to few-shot test inputs, which aligns well.\n",
      "\n",
      "TextCore tasks include ARC [22], PIQA [7], LAMBADA [89], WinoGrande [97], HellaSWAG [129], SciQ [117], TriviaQA [50], and WebQS [6].\n"
     ]
    }
   ],
   "source": [
    "print(page_nodes[7].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "\n",
    "\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=5,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    ")\n",
    "\n",
    "recursive_query_engine = recursive_index.as_query_engine(\n",
    "    similarity_top_k=5, node_postprocessors=[reranker], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Baseline\n",
    "For comparison, we setup a naive RAG pipeline with default parsing and standard chunking, indexing, retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"./data/paper.pdf\"])\n",
    "base_docs = reader.load_data()\n",
    "raw_index = VectorStoreIndex.from_documents(base_docs)\n",
    "raw_query_engine = raw_index.as_query_engine(\n",
    "    similarity_top_k=5, node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ```new LlamaParse``` as pdf data parsing methods and retrieve tables with two different methods\n",
    "we compare base query engine vs recursive query engine with tables\n",
    "\n",
    "### Table Query Task: Queries for Table Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********Basic Query Engine***********\n",
      "The document discusses the methods, analysis, and insights related to the pre-training of a multimodal large language model (LLM). It includes detailed sections on dataset construction, training processes, evaluation metrics, and qualitative examples of the model's capabilities. The dataset comprises interleaved image-text documents and text-only data, with specific filtering and de-duplication processes outlined. Training details cover both pre-training and supervised fine-tuning, while evaluation details include various benchmarks and qualitative assessments of the model's performance in tasks such as counting objects in images and extracting scene text. The document concludes with acknowledgments of contributions from various individuals involved in the research and development process.\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering d71a02c9-3a9b-4abc-98d7-d539c6cc582e: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Tell me over all summary of the document\n",
      "\u001b[0m\n",
      "***********New LlamaParse+ Recursive Retriever Query Engine***********\n",
      "The document provides a comprehensive overview of methods, analysis, and insights related to the pre-training of multimodal large language models (LLMs). It includes detailed sections on dataset construction, which encompasses interleaved image-text data and text-only data, as well as visual instruction tuning data. The training details cover both pre-training and supervised fine-tuning processes. Evaluation details are also outlined, including pre-training evaluation metrics and benchmarks for fine-tuning. Additionally, the document features qualitative examples and acknowledges author contributions. Overall, it serves as a resource for understanding the methodologies and evaluations involved in multimodal LLM pre-training.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me over all summary of the document\"\n",
    "\n",
    "response_1 = raw_query_engine.query(query)\n",
    "print(\"\\n***********Basic Query Engine***********\")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(\"\\n***********New LlamaParse+ Recursive Retriever Query Engine***********\")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********Basic Query Engine***********\n",
      "The 4-shot results for MM1 ablation across different image encoders show varying performance metrics. For the AIM models, the results are 56.6 for AIM 600M, 59.5 for AIM 1B, and 60.9 for AIM 3B. For the CLIP models, the results are 58.7 for CLIP DFN+VeCap ViT-L, 57.0 for CLIP DFN ViT-H, 60.0 for CLIP DFN+VeCap ViT-H, 62.4 for CLIP DFN+VeCap ViT-L with 336 resolution, 62.6 for CLIP DFN+VeCap ViT-H, 62.2 for CLIP OpenAI ViT-L, and 62.5 for CLIP DFN ViT-H.\n",
      "\n",
      "***********New LlamaParse+ Recursive Retriever Query Engine***********\n",
      "The 4-shot results for the MM1 ablation across different image encoders are as follows:\n",
      "\n",
      "- AIM600M: 56.6\n",
      "- AIM1B: 59.5\n",
      "- AIM3B: 60.9\n",
      "- CLIPDFN+VeCap (ViT-L): 58.7\n",
      "- CLIPDFN (ViT-H, 224): 57.0\n",
      "- CLIPDFN+VeCap (ViT-H): 60.0\n",
      "- CLIPDFN+VeCap (ViT-L): 62.6\n",
      "- CLIPDFN+VeCap (ViT-H, 336): 62.4\n",
      "- CLIPOpenAI (ViT-L): 62.2\n",
      "- CLIPDFN (ViT-H, 378): 62.5\n",
      "\n",
      "These results indicate the performance of different models when evaluated with four examples.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about 4-shot result numbers  across all models of MM1 ablation across different image encoders\"\n",
    "\n",
    "response_1 = raw_query_engine.query(query)\n",
    "print(\"\\n***********Basic Query Engine***********\")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(\"\\n***********New LlamaParse+ Recursive Retriever Query Engine***********\")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********Basic Query Engine***********\n",
      "The token size for text-only data in the pre-training data ablation is 2 trillion tokens.\n",
      "\n",
      "***********New LlamaParse+ Recursive Retriever Query Engine***********\n",
      "The token size for the text-only data type in the pre-training data ablation is 2 trillion tokens.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the token size for test only data type  in pre training data ablation?\"\n",
    "\n",
    "response_1 = raw_query_engine.query(query)\n",
    "print(\"\\n***********Basic Query Engine***********\")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(\"\\n***********New LlamaParse+ Recursive Retriever Query Engine***********\")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The 4-shot result numbers across all models in the MM1 ablation with different image encoders are as follows:\\n\\n- Recon.AIM 600M ViT/600M: 56.6\\n- AIM 1B ViT/1B: 59.5\\n- AIM 3B ViT/3B: 60.9\\n- ContrastiveCLIP DFN+VeCap ViT-L: 58.7\\n- CLIP DFN ViT-H DFN-5B: 57.0\\n- CLIP DFN+VeCap ViT-H DFN-5B +VeCap: 60.0\\n- CLIP DFN+VeCap ViT-L DFN-5B +VeCap: 62.4\\n- CLIP DFN+VeCap ViT-H: 62.6\\n- CLIP OpenAI ViT-L ImageText-400M: 62.2\\n- CLIP DFN ViT-H DFN-5B: 62.5\\n\\nThese results indicate the performance of various models when utilizing 4-shot learning across different image encoders.', source_nodes=[NodeWithScore(node=TextNode(id_='69b5761b-7b3e-4158-9539-2d1ff0444773', embedding=None, metadata={'page_label': '6', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a6f83d1-0e7c-4610-b274-8bf7e7ee3d6e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='8d143a9f1eddfea475a64dcd54085fd090fe3af305465f13a415cba7b3e97ec9')}, text='6 B. McKinzie et al.\\nSetup Results\\nModel Arch. Image Res. Data 0-shot 4-shot 8-shot Recon.AIM 600M ViT/600M\\n224 DFN-2B36.6 56.6 60.7\\nAIM 1B ViT/1B 37.9 59.5 63.3\\nAIM 3B ViT/3B 38.9 60.9 64.9ContrastiveCLIP DFN+VeCap ViT-L\\n224DFN-5B +VeCap 36.9 58.7 62.2\\nCLIP DFN ViT-H DFN-5B 37.5 57.0 61.4\\nCLIP DFN+VeCap ViT-H DFN-5B +VeCap 37.5 60.0 63.6\\nCLIP DFN+VeCap ViT-L\\n336DFN-5B +VeCap39.9 62.4 66.0\\nCLIP DFN+VeCap ViT-H 40.5 62.666.3\\nCLIP OpenAI ViT-L ImageText-400M 39.3 62.2 66.1\\nCLIP DFN ViT-H 378 DFN-5B 40.962.5 66.4\\nTable 1: MM1 pre-training ablation across different image encoders (with 2.9B LLM).\\nNote that the values in the Data column correspond to the data that was used for the\\ninitial training of the image encoder itself, not MM1. Recon.: Reconstructive loss.\\nAIM: [30]; DFN-2/5B: [31]; VeCap: VeCap-300M [57]; OpenAI [91].\\nof our ablations, here we use a 2.9B LLM (instead of 1.2B) to ensure there is\\nsufficient capacity to utilize some of the larger image encoders.\\nContrastive losses. Whentrainedonlarge-scaleimage-textdatasets,theresulting\\nmodels possess strong semantic understanding of the image data as evidenced\\nby performance on various forms of image classification and retrieval tasks [91].\\nThese results were enabled because of the availability of large-scale image-text\\ndata, which can endow a visual encoder with semantic knowledge. More recently,\\nautomatically curated large-scale datasets and synthetic captions have led to\\neven stronger encoders [31,57].\\nReconstructive Losses. When it comes to dense prediction, CLIP-style models\\nstruggle to attain the same strong performance [94,95,112]. This property can\\nbe problematic for MLLMs, as many of the tasks such as VQA and captioning\\nrequire detailed image understanding. Hence, we also consider image encoders\\nlearned using reconstructive losses, as such losses explicitly capture all parts of\\nan image. In particular, we utilize AIM [30], which has shown that a carefully\\ndesigned autoregressive reconstructive loss on image data alone scales well.\\nEncoder Lesson: Image resolution has the highest impact, followed\\nby model size and training data composition. As we can see in Table 1,\\nincreasing image resolution from 224to336results in approx. 3%boost in all\\nmetrics across all architectures. Increasing the model size from ViT-L to ViT-H,\\na doubling in parameters, results in a modest performance increase of usually\\nless than 1%. Finally, adding VeCap-300M [57], a dataset of synthetic captions,\\nyields more than 1%boost in few-shot scenarios.\\nWhen it comes to model type, the results are less conclusive. Contrastive\\nmethods tend to result in higher performance than reconstructive. In particular,\\nencodersbasedonViT-Lof300Mparametersresultin 0.3%to1.5%performance\\ngain compared to AIM 600Mof comparable size (only 20 of the 24 AIM model', mimetype='text/plain', start_char_idx=0, end_char_idx=2819, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.393422842025757), NodeWithScore(node=TextNode(id_='2f58d02b-cd45-44c5-b710-4ced7aaece14', embedding=None, metadata={'page_label': '31', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3812e155-8b0a-4972-b44a-e63e83ab2a14', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '31', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='fc99a27e0c73e861fc23dfc04f8e04352f392eaebf8d643cf4d08d16443c244e')}, text='MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training 31\\nSFT-Metrics9698100102104\\n103 .1\\n99.9\\n99.5\\n99.5\\n97.4\\n97.3\\n99.6\\n100Average Performance\\n0/100/0 0/50/50 33/33/33\\n50/50/0 50/0/50 100/0/0\\n42/42/14 45/45/9\\n(a) Interleaved/Captions/Text\\ndata mixture.224px\\n64tks336px\\n64tks336px\\n144tks95100105110115\\n101 .3\\n105 .1\\n105 .2100\\n103 .2\\n106 .2100 .4\\n105 .5\\n107 .8Average SFT-Metrics\\nAvg. Pool Att. Pool\\nC-Abstractor\\n(b)Vision-language connector.336px\\n144tks672px\\n720tks1344px\\n720tks90100110120\\n100\\n109 .8\\n113102 .2\\n109 .6\\n110 .1Average SFT-Metrics\\nUnfrozen Encoder\\nFrozen Encoder\\n(c)(Un-)freezing image encoder\\nduring SFT.\\nFig. 10: SFT ablations. (a) The impact of pre-training data mixture on SFT results.\\nHere, x/y/zmeans that x%of the data is interleaved, y%is captions, and z%is pure\\ntext. tks: the number of image tokens. (b) The impact of different vision-language\\nconnectors on SFT results. For both (a) and (b), we first pre-train MM1-3B with\\nthe ablated setting, and then perform SFT on the pre-trained models. (c) Freezing or\\nunfreezing the image encoder during SFT.\\neach task; that is, we adjust every metric by dividing it by its respective baseline,\\nfollowed by averaging across all metrics. To elaborate, we establish our baseline\\nusing the performance metrics of a compact MM1 model, which is trained on\\n224×224image resolution and employs attention pooling with 64 image queries.\\nC.4 Additional SFT Ablations\\nIn this section, we perform SFT ablations. This section is analogous to Section 3;\\nhere, we perform SFT on the same checkpoints and evaluate if similar lessons\\nhold true on SFT evaluations, instead of pre-training evaluations. Furthermore,\\nwe also study whether to keep the image encoder frozen or not during SFT. For\\nall of these ablations, we train MM1-3B-Chat.\\nPre-training data mixture ablations. In Figure 10a, we compare the SFT\\nperformance with different weights for pre-training data. We see a similar trend\\nwhen comparing with Figure 5 for 0-shot evaluations. Pre-training with caption-\\nonly data gives the best performance across the SFT evaluation metrics. This\\ncorroborates Data lesson 1 : caption data still lifts zero-shot performance for\\nSFT evaluations. However, the SFT metrics do not measure few-shot perfor-\\nmance, so the impact of the interleaved data is not noticeable in this table.\\nVisual-language connector ablations. In Figure 10b, we evaluate different\\nvisual-language connector configurations. This figure is similar to Figure 4, ex-\\ncept that we evaluate the corresponding SFT models. As can be seen, if a low\\nnumber of image tokens is used, average pooling gives similar results as C-\\nAbstractor. When the number of image tokens is increased, the C-Abstractor\\nconfiguration gives the best results. These trends are not entirely consistent\\nwith pre-training results reported in Figure 4. Overall, the impact of the choice', mimetype='text/plain', start_char_idx=0, end_char_idx=2881, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.144640564918518), NodeWithScore(node=TextNode(id_='b103dbef-25a6-4ad6-9b23-cda96549aea2', embedding=None, metadata={'page_label': '32', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2da83b96-3ec0-4971-9f1f-3290bd2e8bd5', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '32', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='a9ec588d837cafd8970c76f25ee3ea8a18a77f509d8405512ec4ae805a9b9e79')}, text='32 B. McKinzie et al.\\nof visual-language connector appears to not have a very significant impact on\\nfinal test performance. Our final models use the C-Abstractor architecture.\\nImage encoder ablations. In Figure 10c, we study whether to keep the image\\nencoder frozen or not during SFT. The results show that at lower image reso-\\nlutions, a frozen image encoder results in better performance than an unfrozen\\nimage encoder (+2.2 points). However, at higher resolutions ( i.e., 1344px), it\\nis beneficial to unfreeze the image encoder (+2.9 points). This is likely because\\nthe pre-training is performed at the base resolution without any interpolation\\nor image sub-divisions.\\nC.5 Implementation Details for Few-shot MM1-30B-Chat\\nAs shown in Section 5.1, our fine-tuned model can utilize in-context examples to\\nachieve even stronger performance. Interestingly, the performance goes up when\\nincreasing the number of examples. We demonstrate this with MM1-30B-Chat.\\nOne challenge for few-shot inputs arises due to the use of sub-image decom-\\nposition.Whilethisstrategyliftszero-shotperformance,itsignificantlyincreases\\ntheeffectivenumberoftokensconsumedperimage.Using5sub-imagesperinput\\nimage as MM1-30B-Chat does, processing a 4-shot example where every example\\ncontains just one source image already yields 20 effective images. Representing\\nevery image with 144 tokens therefore requires 2,880 tokens for images alone,\\nquickly exhausting limited language model context. To mitigate this limitation,\\nwe propose a new mixed-resolution approach. Specifically, for Kin-context ex-\\namples, we only encode the last Nimages at a high resolution via sub-image\\ndecomposition, the remaining K−Nin-context examples are processed at lower\\nresolution. This makes in-context examples much less expensive to encode and\\nallowstoincreasethenumberofin-contextexampleswithinafixedtokenbudget,\\nfurther increasing performance. In our implementation, we set N= 3.\\nWe demonstrate the effectiveness of our proposed strategy using the Math-\\nVista benchmark. Using in-context learning with chain-of-thought, the perfor-\\nmance of MM1-30B-Chat improves from 39.4 (0-shot) to 41.9 (4-shot, with all\\nin-context examples using full sub-image decomposition). Applying our mixed-\\nresolution approach allows to encode additional in-context examples, enabling\\nup to 8-shot chain of thought, further improving the performance to 44.4. This\\nillustrates that our MM1-Chat model retains in-context learning capabilities in-\\nherited from its pre-training regimen, and that our strategy of mixed decompo-\\nsition is effective at further increasing few-shot performance within a restricted\\ncontext length.', mimetype='text/plain', start_char_idx=0, end_char_idx=2657, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4937745928764343), NodeWithScore(node=TextNode(id_='acb2d2a6-bf37-4103-8f35-b1cd9a91454f', embedding=None, metadata={'page_label': '28', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='41d9477c-db44-4351-acfe-4fac2176652a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '28', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='8be94b10ee53a0d67d4618fc8648d34e0f2517ac9e47f869ac7fac55e6cd5e90'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c40f3e36-9231-47ba-83d8-dc1c6c13d688', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '28', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='9174e944029fdd25d46ef6454a842079ec669f0e11885a1186fa09a830a23802')}, text='3 – –56.3 35.0 31.6 50.6\\n4103.2 – –63.1 36.5 39.6 57.4\\n8108.8 – –65.6 37.3 44.8 57.5\\n16110.5 – –66.8 37.6 48.4 57.8\\nEmu2-37B [105]0–– –33.3 26.2 40.4 26.7\\n4–– –67.0 48.2 54.6 53.2\\n8–– –67.8 49.3 54.7 54.1\\n16–– –68.8 50.3 57.0 57.1\\nMM1-30B070.354.6 64.9 48.9 28.2 14.5 24.1\\n4117.9103.8 87.5 68.8 48.1 41.7 54.9\\n8123.1111.6 92.9 70.9 49.4 49.9 58.3\\n16125.3116.0 97.6 71.9 50.6 57.9 59.3\\nTable 6: Complete MM1 pre-training few-shot evaluation results. (*) IDEFICS in-\\ncludes PMD in its training data (includes COCO). ( †) These models included two\\ntext-only demonstrations in their “0” prompt, whereas MM1 does not.', mimetype='text/plain', start_char_idx=1194, end_char_idx=1806, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-2.776116132736206), NodeWithScore(node=TextNode(id_='6c8c3fd0-4462-4872-8489-f36a62bcd8d5', embedding=None, metadata={'page_label': '12', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='eee9bd4f-75b8-4157-8699-c2a02c21b017', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, hash='af4a3570f9fde60c21991a3e8b81449c7c1ca227a054e162550cb279918c8a30'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c04a4926-3d8c-4c15-8194-95889d2846c3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='e1c92a969aadb272b29bc051745aea112d94ab9a4a209a7d41d9dcb81fd7c633')}, text='12 B. McKinzie et al.\\nModel ShotCaptioning Visual Question Answering\\nCOCO NoCaps TextCaps VQAv2 TextVQA VizWiz OKVQA\\nMM1-3B Model Comparisons\\nFlamingo-3B [3]0†73.0 – – 49.2 30.1 28.9 41.2\\n890.6 – –55.4 32.4 38.4 44.6\\nMM1-3B0 73.5 55.6 63.3 46.2 29.4 15.6 26.1\\n8114.6 104.7 88.8 63.6 44.6 46.4 48.4\\nMM1-7B Model Comparisons\\nIDEFICS-9B [58]0†46.0* 36.8 25.4 50.9 25.9 35.5 38.4\\n897.0* 86.8 63.2 56.4 27.5 40.4 47.7\\nFlamingo-9B [3]0†79.4 – – 51.8 31.8 28.8 44.7\\n899.0 – –58.0 33.6 39.4 50.0\\nEmu2-14B [105]0†– – – 52.9 – 34.4 42.8\\n8–– –59.0 –43.9 –\\nMM1-7B0 76.3 61.0 64.2 47.8 28.8 15.6 22.6\\n8116.3 106.6 88.2 63.6 46.3 45.3 51.4\\nMM1-30B Model Comparisons\\nIDEFICS-80B [58]0†91.8* 65.0 56.8 60.0 30.9 36.0 45.2\\n8114.3* 105.7 77.6 64.8 35.7 46.1 55.1\\n16116.6* 107.0 81.4 65.4 36.3 48.3 56.8\\nFlamingo-80B [3]0†84.3 – – 56.3 35.0 31.6 50.6\\n8108.8 – –65.6 37.3 44.8 57.5\\n16110.5 – –66.8 37.6 48.4 57.8\\nEmu2-37B [105]0 – – – 33.3 26.2 40.4 26.7\\n8–– –67.8 49.3 54.7 54.1\\n16–– –68.8 50.3 57.0 57.1\\nMM1-30B0 70.3 54.6 64.9 48.9 28.2 14.5 24.1\\n8123.1111.6 92.9 70.9 49.4 49.9 58.3\\n16 125.3 116.0 97.6 71.9 50.6 57.9 59.3\\nTable 3: Multimodal pre-training evaluations. (*) IDEFICS includes PMD in its train-\\ning data (includes COCO). ( †) These models include two text-only demonstrations in\\ntheir “0” prompt, whereas MM1 does not. For the full table, see Table 6 in Appendix.\\nshot, as shown in Table 3, and compare against the few approaches that report\\nfew-shot pre-training performance. Note that we only compare our model with\\nlarger models, e.g., comparing our 30B model with two 80B models.\\nWhen it comes to few-shot performance, MM1 outperforms all published\\nprior work for pre-trained MLLMs. We see superior performance at 30B across\\ncaptioning benchmarks and the VizWiz-QA benchmark. On VQAv2, TextVQA,\\nOKVQA, at that scale we are comparable to Emu2 [105].', mimetype='text/plain', start_char_idx=0, end_char_idx=1849, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-3.0822861194610596)], metadata={'69b5761b-7b3e-4158-9539-2d1ff0444773': {'page_label': '6', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, '2f58d02b-cd45-44c5-b710-4ced7aaece14': {'page_label': '31', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, 'b103dbef-25a6-4ad6-9b23-cda96549aea2': {'page_label': '32', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, 'acb2d2a6-bf37-4103-8f35-b1cd9a91454f': {'page_label': '28', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}, '6c8c3fd0-4462-4872-8489-f36a62bcd8d5': {'page_label': '12', 'file_name': 'paper.pdf', 'file_path': 'data\\\\paper.pdf', 'file_type': 'application/pdf', 'file_size': 18782815, 'creation_date': '2024-10-30', 'last_modified_date': '2024-10-29'}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********Basic Query Engine***********\n",
      "The Japanese document digitization pipeline utilizes LayoutParser to generate high-quality structured data from historical Japanese firm financial tables, which often feature complicated layouts. This pipeline employs two layout detection models to identify different levels of document structures and utilizes two customized OCR engines to enhance character recognition accuracy.\n",
      "\n",
      "The documents typically contain vertically arranged text columns, a common format in Japanese writing. Due to issues like scanning noise and variations in printing technology, these columns can be skewed or have inconsistent widths, making them challenging to identify with traditional rule-based methods. The pipeline addresses these challenges by implementing a document reorganization algorithm that rearranges detected tokens based on their bounding boxes, improving character recognition recall.\n",
      "\n",
      "Additionally, the pipeline is designed to handle unique fonts and glyphs used in historical documents, which can degrade the performance of standard OCR models. By training a layout model to identify specific regions for numbers and using a self-trained OCR model, the pipeline achieves high recognition accuracy, demonstrating its effectiveness in processing complex historical documents. Overall, this approach allows for the creation of a robust and intricate digitization pipeline tailored to the unique characteristics of Japanese documents.\n",
      "\n",
      "***********New LlamaParse+ Recursive Retriever Query Engine***********\n",
      "A comprehensive pipeline was developed to digitize historical Japanese firm financial tables, which often feature complicated layouts. This pipeline utilizes two layout models to identify various levels of document structures and incorporates two customized OCR engines to enhance character recognition accuracy. The documents typically contain vertically written text, a common format in Japanese, which can present challenges due to scanning noise and the variability in column widths. The pipeline effectively addresses these complexities to generate high-quality structured data from the historical documents.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about japanese documenty pipeline\"\n",
    "\n",
    "response_1 = raw_query_engine.query(query)\n",
    "print(\"\\n***********Basic Query Engine***********\")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(\"\\n***********New LlamaParse+ Recursive Retriever Query Engine***********\")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The Japanese document digitization pipeline utilizes LayoutParser to generate high-quality structured data from historical Japanese firm financial tables, which often feature complicated layouts. This pipeline employs two layout detection models to identify different levels of document structures and utilizes two customized OCR engines to enhance character recognition accuracy.\\n\\nThe documents typically contain vertically arranged text columns, a common format in Japanese writing. Due to issues like scanning noise and variations in printing technology, these columns can be skewed or have inconsistent widths, making them challenging to identify with traditional rule-based methods. The pipeline addresses these challenges by implementing a document reorganization algorithm that rearranges detected tokens based on their bounding boxes, improving character recognition recall.\\n\\nAdditionally, the pipeline is designed to handle unique fonts and glyphs used in historical documents, which can degrade the performance of standard OCR models. By training a layout model to identify specific regions for numbers and using a self-trained OCR model, the pipeline achieves high recognition accuracy, demonstrating its effectiveness in processing complex historical documents. Overall, this approach allows for the creation of a robust and intricate digitization pipeline tailored to the unique characteristics of Japanese documents.', source_nodes=[NodeWithScore(node=TextNode(id_='2258d1d7-0939-4dc4-9cc0-5bd3224d1f65', embedding=None, metadata={'page_label': '11', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3ea607f9-e3dc-4e18-ab73-837263056231', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, hash='bd43d3ebe28ccd0ced9ee0d7c7c2111ede4ed0c8c196267e1410c712a2709850')}, text='LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 11\\nfocuses on precision, eﬃciency, and robustness. The target documents may have\\ncomplicated structures, and may require training multiple layout detection models\\nto achieve the optimal accuracy. Light-weight pipelines are built for relatively\\nsimple documents, with an emphasis on development ease, speed and ﬂexibility.\\nIdeally one only needs to use existing resources, and model training should be\\navoided. Through two exemplar projects, we show how practitioners in both\\nacademia and industry can easily build such pipelines using LayoutParser and\\nextract high-quality structured document data for their downstream tasks. The\\nsource code for these projects will be publicly available in the LayoutParser\\ncommunity hub.\\n5.1 A Comprehensive Historical Document Digitization Pipeline\\nThe digitization of historical documents can unlock valuable data that can shed\\nlight on many important social, economic, and historical questions. Yet due to\\nscan noises, page wearing, and the prevalence of complicated layout structures, ob-\\ntaining a structured representation of historical document scans is often extremely\\ncomplicated.\\nFig. 5: Illustration of how LayoutParser\\nhelps with the historical document digi-\\ntization pipeline.In this example, LayoutParser was\\nused to develop a comprehensive\\npipeline, shown in Figure 5, to gener-\\nate high-quality structured data from\\nhistorical Japanese ﬁrm ﬁnancial ta-\\nbles with complicated layouts. The\\npipeline applies two layout models to\\nidentify diﬀerent levels of document\\nstructures and two customized OCR\\nengines for optimized character recog-\\nnition accuracy.\\nAs shown in Figure 4 (a), the\\ndocument contains columns of text\\nwritten vertically15, a common style\\nin Japanese. Due to scanning noise\\nand archaic printing technology, the\\ncolumns can be skewed or have vari-\\nable widths, and hence cannot be eas-\\nily identiﬁed via rule-based methods.\\nWithin each column, words are sepa-\\nrated by white spaces of variable size,\\nand the vertical positions of objects\\ncan be an indicator of their layout\\ntype.\\n15A document page consists of eight rows like this. For simplicity we skip the row\\nsegmentation discussion and refer readers to the source code when available.', mimetype='text/plain', start_char_idx=0, end_char_idx=2254, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-1.1931051015853882), NodeWithScore(node=TextNode(id_='2289613a-6438-42c9-8d3b-21441dc1d7ac', embedding=None, metadata={'page_label': '10', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c116af2-78ea-405c-9114-268faabd41a8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, hash='69a3530678a429351aeb864605d8a37f87fc77a78767b8caf31bf9410b6a7741')}, text='10 Z. Shen et al.\\nFig. 4: Illustration of (a) the original historical Japanese document with layout\\ndetection results and (b) a recreated version of the document image that achieves\\nmuch better character recognition recall. The reorganization algorithm rearranges\\nthe tokens based on the their detected bounding boxes given a maximum allowed\\nheight.\\n4LayoutParser Community Platform\\nAnother focus of LayoutParser is promoting the reusability of layout detection\\nmodels and full digitization pipelines. Similar to many existing deep learning\\nlibraries, LayoutParser comes with a community model hub for distributing\\nlayout models. End-users can upload their self-trained models to the model hub,\\nand these models can be loaded into a similar interface as the currently available\\nLayoutParser pre-trained models. For example, the model trained on the News\\nNavigator dataset [17] has been incorporated in the model hub.\\nBeyond DL models, LayoutParser also promotes the sharing of entire doc-\\nument digitization pipelines. For example, sometimes the pipeline requires the\\ncombination of multiple DL models to achieve better accuracy. Currently, pipelines\\nare mainly described in academic papers and implementations are often not pub-\\nlicly available. To this end, the LayoutParser community platform also enables\\nthe sharing of layout pipelines to promote the discussion and reuse of techniques.\\nFor each shared pipeline, it has a dedicated project page, with links to the source\\ncode, documentation, and an outline of the approaches. A discussion panel is\\nprovided for exchanging ideas. Combined with the core LayoutParser library,\\nusers can easily build reusable components based on the shared pipelines and\\napply them to solve their unique problems.\\n5 Use Cases\\nThe core objective of LayoutParser is to make it easier to create both large-scale\\nand light-weight document digitization pipelines. Large-scale document processing', mimetype='text/plain', start_char_idx=0, end_char_idx=1925, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-2.151923179626465), NodeWithScore(node=TextNode(id_='f5f3d61f-2c30-4514-a602-5ccb4376abce', embedding=None, metadata={'page_label': '4', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0efddbc1-b2d4-4576-b14f-9ff99c94f07a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, hash='67c62c1cc3ae9ed4960f4f9040b1547d5a52f18deef6aa3968817dd37fc3c21a')}, text='4 Z. Shen et al.\\nEfficient Data AnnotationC u s t o m i z e d  M o d e l  T r a i n i n gModel Cust omizationDI A Model HubDI A Pipeline SharingCommunity PlatformLa y out Detection ModelsDocument Images \\nT h e  C o r e  L a y o u t P a r s e r  L i b r a r yOCR ModuleSt or age & VisualizationLa y out Data Structur e\\nFig. 1: The overall architecture of LayoutParser . For an input document image,\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [ 8] and transformers [ 34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser , on the other\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nandTensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [ 3](magazine layouts),\\nPubLayNet [ 38](academic paper layouts), Table Bank [ 18](tables in academic\\npapers), Newspaper Navigator Dataset [ 16,17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3 The Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered', mimetype='text/plain', start_char_idx=0, end_char_idx=2429, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-2.618896245956421), NodeWithScore(node=TextNode(id_='a55d7d96-dc8e-4541-9011-fa7b733fc6a0', embedding=None, metadata={'page_label': '2', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f0957837-28e5-43dd-9c51-0db1b7911bc1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, hash='66b62c88613bed33599a136f0feaaae86880ef4dda9aebea7c807a095fd8e1c6')}, text='2 Z. Shen et al.\\n37], layout detection [ 38,22], table detection [ 26], and scene text detection [ 4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [ 1] or PyTorch [ 24], and the high-level parameters can\\nbe obfuscated by implementation details [ 8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,\\nand these pipelines are not documented in any central location (and often not\\ndocumented at all). This makes it diﬃcult for research teams to learn about how\\nfull pipelines are implemented and leads them to invest signiﬁcant resources in\\nreinventing the DIA wheel .\\nLayoutParser provides a uniﬁed toolkit to support DL-based document image\\nanalysis and processing. To address the aforementioned challenges, LayoutParser\\nis built with the following components:\\n1.An oﬀ-the-shelf toolkit for applying DL models for layout detection, character\\nrecognition, and other DIA tasks (Section 3)\\n2.A rich repository of pre-trained neural network models (Model Zoo) that\\nunderlies the oﬀ-the-shelf usage\\n3.Comprehensive tools for eﬃcient document image data annotation and model\\ntuning to support diﬀerent levels of customization\\n4.A DL model hub and community platform for the easy sharing, distribu-\\ntion, and discussion of DIA models and pipelines, to promote reusability,\\nreproducibility, and extensibility (Section 4)\\nThe library implements simple and intuitive Python APIs without sacriﬁcing\\ngeneralizability and versatility, and can be easily installed via pip. Its convenient\\nfunctions for handling document image data can be seamlessly integrated with\\nexisting DIA pipelines. With detailed documentations and carefully curated\\ntutorials, we hope this tool will beneﬁt a variety of end-users, and will lead to\\nadvances in applications in both industry and academic research.\\nLayoutParser is well aligned with recent eﬀorts for improving DL model\\nreusability in other disciplines like natural language processing [ 8,34] and com-\\nputer vision [ 35], but with a focus on unique challenges in DIA. We show\\nLayoutParser can be applied in sophisticated and large-scale digitization projects', mimetype='text/plain', start_char_idx=0, end_char_idx=3275, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-7.725473880767822), NodeWithScore(node=TextNode(id_='07b8a3d2-afde-45e4-9377-0dabef13e866', embedding=None, metadata={'page_label': '12', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='66431add-0053-48bf-b15c-1d5c081543fd', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, hash='b8bd2719c598a534edecead17c793eefa2d9816bbbe2638f4136a12240c75f06')}, text='12 Z. Shen et al.\\nTo decipher the complicated layout\\nstructure, two object detection models have been trained to recognize individual\\ncolumns and tokens, respectively. A small training set (400 images with approxi-\\nmately 100 annotations each) is curated via the active learning based annotation\\ntool [ 32] inLayoutParser . The models learn to identify both the categories and\\nregions for each token or column via their distinct visual features. The layout\\ndata structure enables easy grouping of the tokens within each column, and\\nrearranging columns to achieve the correct reading orders based on the horizontal\\nposition. Errors are identiﬁed and rectiﬁed via checking the consistency of the\\nmodel predictions. Therefore, though trained on a small dataset, the pipeline\\nachieves a high level of layout detection accuracy: it achieves a 96.97 AP [ 19]\\nscore across 5 categories for the column detection model, and a 89.23 AP across\\n4 categories for the token detection model.\\nA combination of character recognition methods is developed to tackle the\\nunique challenges in this document. In our experiments, we found that irregular\\nspacing between the tokens led to a low character recognition recall rate, whereas\\nexisting OCR models tend to perform better on densely-arranged texts. To\\novercome this challenge, we create a document reorganization algorithm that\\nrearranges the text based on the token bounding boxes detected in the layout\\nanalysis step. Figure 4 (b) illustrates the generated image of dense text, which is\\nsent to the OCR APIs as a whole to reduce the transaction costs. The ﬂexible\\ncoordinate system in LayoutParser is used to transform the OCR results relative\\nto their original positions on the page.\\nAdditionally, it is common for historical documents to use unique fonts\\nwith diﬀerent glyphs, which signiﬁcantly degrades the accuracy of OCR models\\ntrained on modern texts. In this document, a special ﬂat font is used for printing\\nnumbers and could not be detected by oﬀ-the-shelf OCR engines. Using the highly\\nﬂexible functionalities from LayoutParser , a pipeline approach is constructed\\nthat achieves a high recognition accuracy with minimal eﬀort. As the characters\\nhave unique visual structures and are usually clustered together, we train the\\nlayout model to identify number regions with a dedicated category. Subsequently,\\nLayoutParser crops images within these regions, and identiﬁes characters within\\nthem using a self-trained OCR model based on a CNN-RNN [ 6]. The model\\ndetects a total of 15 possible categories, and achieves a 0.98 Jaccard score16and\\na 0.17 average Levinstein distances17for token prediction on the test set.\\nOverall, it is possible to create an intricate and highly accurate digitization\\npipeline for large-scale digitization using LayoutParser . The pipeline avoids\\nspecifying the complicated rules used in traditional methods, is straightforward\\nto develop, and is robust to outliers. The DL models also generate ﬁne-grained\\nresults that enable creative approaches like page reorganization for OCR.\\n16This measures the overlap between the detected and ground-truth characters, and\\nthe maximum is 1.\\n17This measures the number of edits from the ground-truth text to the predicted text,\\nand lower is better.', mimetype='text/plain', start_char_idx=0, end_char_idx=3260, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-7.787989139556885)], metadata={'2258d1d7-0939-4dc4-9cc0-5bd3224d1f65': {'page_label': '11', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, '2289613a-6438-42c9-8d3b-21441dc1d7ac': {'page_label': '10', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, 'f5f3d61f-2c30-4514-a602-5ccb4376abce': {'page_label': '4', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, 'a55d7d96-dc8e-4541-9011-fa7b733fc6a0': {'page_label': '2', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}, '07b8a3d2-afde-45e4-9377-0dabef13e866': {'page_label': '12', 'file_name': 'layout-parser-paper.pdf', 'file_path': '..\\\\data\\\\layout-parser-paper.pdf', 'file_type': 'application/pdf', 'file_size': 4686220, 'creation_date': '2024-10-28', 'last_modified_date': '2024-10-26'}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='A comprehensive pipeline was developed to digitize historical Japanese firm financial tables, which often feature complicated layouts. This pipeline utilizes two layout models to identify various levels of document structures and incorporates two customized OCR engines to enhance character recognition accuracy. The documents typically contain vertically written text, a common format in Japanese, which can present challenges due to scanning noise and the variability in column widths. The pipeline effectively addresses these complexities to generate high-quality structured data from the historical documents.', source_nodes=[NodeWithScore(node=TextNode(id_='12361d36-1cb9-4f44-a5a0-89ccff52aff0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# LayoutParser: A Unified Toolkit for DL-Based DIA\\n\\nfocuses on precision, efficiency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and flexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.\\n\\n# 5.1 A Comprehensive Historical Document Digitization Pipeline\\n\\nThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, obtaining a structured representation of historical document scans is often extremely complicated.\\n\\nIn this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to generate high-quality structured data from historical Japanese firm financial tables with complicated layouts. The pipeline applies two layout models to identify different levels of document structures and two customized OCR engines for optimized character recognition accuracy.\\n\\nAs shown in Figure 4 (a), the document contains columns of text written vertically15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have variable widths, and hence cannot be easily identified via rule-based methods. Within each column, words are separated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.\\n\\n15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\n\\nFig. 5: Illustration of how LayoutParser helps with the historical document digitization pipeline.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-0.8220428824424744), NodeWithScore(node=TextNode(id_='176199c8-ef76-4ef4-9553-934abb839493', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='525b8e3b-7ffa-47df-b30d-c59e7f2d3382', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='4dd5fafa720b92431f056cb94cd2bf2aaa823694726494a8ded0531b973c7435')}, text='LayoutParser: A Unified Toolkit for DL-Based DIA\\n\\nfocuses on precision, efficiency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and flexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.\\n\\n 5.1 A Comprehensive Historical Document Digitization Pipeline\\n\\nThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, obtaining a structured representation of historical document scans is often extremely complicated.\\n\\nIn this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to generate high-quality structured data from historical Japanese firm financial tables with complicated layouts. The pipeline applies two layout models to identify different levels of document structures and two customized OCR engines for optimized character recognition accuracy.\\n\\nAs shown in Figure 4 (a), the document contains columns of text written vertically15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have variable widths, and hence cannot be easily identified via rule-based methods. Within each column, words are separated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.\\n\\n15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\n\\nFig. 5: Illustration of how LayoutParser helps with the historical document digitization pipeline.', mimetype='text/plain', start_char_idx=1, end_char_idx=2253, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-0.9155403971672058), NodeWithScore(node=TextNode(id_='dabb7b62-c60a-4b62-80df-b94073403170', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 4 Z. Shen et al.\\n\\n# Model Customization\\n\\n- Efficient Data Annotation\\n- Customized Model Training\\n- OCR Module\\n\\n# Document Images\\n\\n- Layout Detection Models\\n- Layout Data Structure\\n\\n# Community Platform\\n\\n- DIA Model Hub\\n- DIA Pipeline Sharing\\n- Storage & Visualization\\n\\n# The Core LayoutParser Library\\n\\nFig. 1: The overall architecture of LayoutParser. For an input document image, the core LayoutParser library provides a set of off-the-shelf tools for layout detection, OCR, visualization, and storage, backed by a carefully designed layout data structure. LayoutParser also supports high level customization via efficient layout annotation and model training functions. These improve model accuracy on the target samples. The community platform enables the easy sharing of DIA models and whole digitization pipelines to promote reusability and reproducibility. A collection of detailed documentation, tutorials and exemplar projects make LayoutParser easy to learn and use.\\n\\nAllenNLP [8] and transformers [34] have provided the community with complete DL-based support for developing and deploying models for general computer vision and natural language processing problems. LayoutParser, on the other hand, specializes specifically in DIA tasks. LayoutParser is also equipped with a community platform inspired by established model hubs such as Torch Hub [23] and TensorFlow Hub [1]. It enables the sharing of pretrained models as well as full document processing pipelines that are unique to DIA tasks.\\n\\nThere have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3] (magazine layouts), PubLayNet [38] (academic paper layouts), Table Bank [18] (tables in academic papers), Newspaper Navigator Dataset [16, 17] (newspaper figure layouts) and HJDataset [31] (historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support different use cases.\\n\\n# 3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an off-the-shelf toolkit that streamlines DL-based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-2.821235418319702), NodeWithScore(node=TextNode(id_='60608bd6-8bef-4efa-a28f-31f93a2f896b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10cde4dc-436c-4d82-8dad-e1d1153b5a81', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='cf5ce461d04009151467ecccc5152f896fbffe020a97f86afe88e008c2515f9f')}, text='4 LayoutParser Community Platform\\n\\nAnother focus of LayoutParser is promoting the reusability of layout detection models and full digitization pipelines. Similar to many existing deep learning libraries, LayoutParser comes with a community model hub for distributing layout models. End-users can upload their self-trained models to the model hub, and these models can be loaded into a similar interface as the currently available LayoutParser pre-trained models. For example, the model trained on the News Navigator dataset [17] has been incorporated in the model hub.\\n\\nBeyond DL models, LayoutParser also promotes the sharing of entire document digitization pipelines. For example, sometimes the pipeline requires the combination of multiple DL models to achieve better accuracy. Currently, pipelines are mainly described in academic papers and implementations are often not publicly available. To this end, the LayoutParser community platform also enables the sharing of layout pipelines to promote the discussion and reuse of techniques. For each shared pipeline, it has a dedicated project page, with links to the source code, documentation, and an outline of the approaches. A discussion panel is provided for exchanging ideas. Combined with the core LayoutParser library, users can easily build reusable components based on the shared pipelines and apply them to solve their unique problems.\\n\\n 5 Use Cases\\n\\nThe core objective of LayoutParser is to make it easier to create both large-scale and light-weight document digitization pipelines. Large-scale document processing', mimetype='text/plain', start_char_idx=1, end_char_idx=1578, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-7.754135608673096), NodeWithScore(node=TextNode(id_='514578f1-1f1f-407e-831b-ba00a2747b4c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 4 LayoutParser Community Platform\\n\\nAnother focus of LayoutParser is promoting the reusability of layout detection models and full digitization pipelines. Similar to many existing deep learning libraries, LayoutParser comes with a community model hub for distributing layout models. End-users can upload their self-trained models to the model hub, and these models can be loaded into a similar interface as the currently available LayoutParser pre-trained models. For example, the model trained on the News Navigator dataset [17] has been incorporated in the model hub.\\n\\nBeyond DL models, LayoutParser also promotes the sharing of entire document digitization pipelines. For example, sometimes the pipeline requires the combination of multiple DL models to achieve better accuracy. Currently, pipelines are mainly described in academic papers and implementations are often not publicly available. To this end, the LayoutParser community platform also enables the sharing of layout pipelines to promote the discussion and reuse of techniques. For each shared pipeline, it has a dedicated project page, with links to the source code, documentation, and an outline of the approaches. A discussion panel is provided for exchanging ideas. Combined with the core LayoutParser library, users can easily build reusable components based on the shared pipelines and apply them to solve their unique problems.\\n\\n# 5 Use Cases\\n\\nThe core objective of LayoutParser is to make it easier to create both large-scale and light-weight document digitization pipelines. Large-scale document processing', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-7.760446071624756)], metadata={'12361d36-1cb9-4f44-a5a0-89ccff52aff0': {}, '176199c8-ef76-4ef4-9553-934abb839493': {}, 'dabb7b62-c60a-4b62-80df-b94073403170': {}, '60608bd6-8bef-4efa-a28f-31f93a2f896b': {}, '514578f1-1f1f-407e-831b-ba00a2747b4c': {}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
