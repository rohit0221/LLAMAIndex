{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model\n",
    "Here we use gpt-4o and default OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 06436396-ce34-425a-a749-0b8c4dd790a8\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "docs = LlamaParse(result_type=\"text\").load_data(\"./attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "def get_sub_docs(docs):\n",
    "    \"\"\"Split docs into pages, by separator.\"\"\"\n",
    "    sub_docs = []\n",
    "    for doc in docs:\n",
    "        doc_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for doc_chunk in doc_chunks:\n",
    "            sub_doc = Document(\n",
    "                text=doc_chunk,\n",
    "                metadata=deepcopy(doc.metadata),\n",
    "            )\n",
    "            sub_docs.append(sub_doc)\n",
    "\n",
    "    return sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will split into pages\n",
    "sub_docs = get_sub_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Graph Store\n",
    "Here we use Neo4j but you can also use our other integrations like Nebula (see an example notebook).\n",
    "\n",
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command\n",
    "\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "From here, you can open the db at http://localhost:7474/. On this page, you will be asked to sign in. Use the default username/password of neo4j and neo4j.\n",
    "\n",
    "Once you login for the first time, you will be asked to change the password.\n",
    "\n",
    "After this, you are ready to create your first property graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.environ.get('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.environ.get('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neo4j+s://cd0238b0.databases.neo4j.io'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEO4J_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neo4j'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEO4J_USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POlRhsDkML7DgHWr8ocEDDsRPcMHDECUK6hzAOcgb94'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEO4J_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "graph_store = Neo4jPGStore(\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Github\\LlamaIndex\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 15/15 [00:00<00:00, 326.07it/s]\n",
      "Extracting implicit paths: 100%|██████████| 16/16 [00:00<00:00, 16031.74it/s]\n",
      "Extracting paths from text: 100%|██████████| 16/16 [00:12<00:00,  1.30it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "index = PropertyGraphIndex.from_documents(\n",
    "    sub_docs,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this if index is already loaded !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PropertyGraphIndex.from_existing(\n",
    "    graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vector Retriever\n",
    "Here we define our vector context retriever - it returns initial nodes via vector search, and traverses the relations to pull in more nodes/context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "\n",
    "kg_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    similarity_top_k=2,\n",
    "    path_depth=1,\n",
    "    # include_text=False,\n",
    "    include_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ">> IDX: 0, Here are some facts extracted from the provided text:\n",
      "\n",
      "Decoder -> Composed of -> Stack of n = 6 identical layers\n",
      "Encoder -> Composed of -> Stack of n = 6 identical layers\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as\n",
      " depicted in Figure 2.\n",
      " Multi-head attention allows the model to jointly attend to information from different representation\n",
      " subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "                                 MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
      "                                                where headi = Attention(QW i Q, KW i K, V W i V)\n",
      "\n",
      "Where the projections are parameter matrices W i ∈ Rdmodel×dk , W i ∈ Rdmodel×dk , W i ∈ Rdmodel×dvQ        K  V\n",
      " and W O ∈ Rhdv ×dmodel .\n",
      " In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      " dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
      " is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      " 3.2.3       Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "            • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "               and the memory keys and values come from the output of the encoder. This allows every\n",
      "               position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "               typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "               [38, 2, 9].\n",
      "            • The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "               and queries come from the same place, in this case, the output of the previous layer in the\n",
      "               encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "               encoder.\n",
      "            • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "               all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "               information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "               inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "               of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      " 3.3      Position-wise Feed-Forward Networks\n",
      " In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      " connected feed-forward network, which is applied to each position separately and identically. This\n",
      " consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "                                                 FFN(x) = max(0, xW1 + b1)W2 + b2                                 (2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      " from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      " df f = 2048.\n",
      "\n",
      " 3.4      Embeddings and Softmax\n",
      "\n",
      " Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      " tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      " mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      " our model, we share the same weight matrix between the two embedding layers and the pre-softmax.\n",
      " linear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel\n",
      "\n",
      "                                                                                5\n",
      ">> IDX: 1, Here are some facts extracted from the provided text:\n",
      "\n",
      "Convolutional layer -> Requires -> Stack of o(n/k\n",
      "\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\n",
      " sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      " [38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      " the input sequence centered around the respective output position. This would increase the maximum\n",
      " path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      " positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      " or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      " between any two positions in the network. Convolutional layers are generally more expensive than\n",
      " recurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\n",
      " considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
      " convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      " the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      " from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      " heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      " and semantic structure of the sentences.\n",
      "\n",
      " 5    Training\n",
      "This section describes the training regime for our models.\n",
      "\n",
      " 5.1    Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      " sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
      " target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      " 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      " batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      " target tokens.\n",
      "\n",
      " 5.2    Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      " the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      " trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      " bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "\n",
      " 5.3    Optimizer\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      " rate over the course of training, according to the formula:\n",
      "\n",
      "                  lrate = dmodel −0.5· min(step_num−0.5, step_num · warmup_steps−1.5)                      (3)\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      " and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      " warmup_steps = 4000.\n",
      "\n",
      " 5.4    Regularization\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "                                                                7\n"
     ]
    }
   ],
   "source": [
    "nodes = kg_retriever.retrieve(\n",
    "    \"explain me Encoder and Decoder Stacks\"\n",
    ")\n",
    "# nodes = kg_retriever.retrieve('san francisco')\n",
    "print(len(nodes))\n",
    "for idx, node in enumerate(nodes):\n",
    "    print(f\">> IDX: {idx}, {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Baseline Vector Index\n",
    "We also build a \"baseline\" vector index. This follows the \"naive\" RAG pipeline approach of chunking and vector embedding. We use this as a comparison point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "base_index = VectorStoreIndex.from_documents(sub_docs, embed_model=embed_model)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=2)\n",
    "base_query_engine = RetrieverQueryEngine(base_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for MultiHead attention is given by:\n",
      "\n",
      "\\[ \n",
      "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O \n",
      "\\]\n",
      "\n",
      "where \n",
      "\n",
      "\\[ \n",
      "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V) \n",
      "\\]\n",
      "\n",
      "In this, \\( W_i^Q \\), \\( W_i^K \\), and \\( W_i^V \\) are parameter matrices, and \\( W_O \\) is another parameter matrix used for the final projection.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"give me formulae for MultiHead\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum path length for self-attention is O(1).\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what is Maximum Path Length for Self-Attention\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                                          | EN-DE | EN-FR |\n",
      "|------------------------------------------------|-------|-------|\n",
      "| ByteNet                                       | 23.75 |       |\n",
      "| Deep-Att + PosUnk                             |       | 39.2  |\n",
      "| GNMT + RL                                     | 24.6  | 39.92 |\n",
      "| ConvS2S                                       | 25.16 | 40.46 |\n",
      "| MoE                                           | 26.03 | 40.56 |\n",
      "| Deep-Att + PosUnk Ensemble                     |       | 40.4  |\n",
      "| GNMT + RL Ensemble                             | 26.30 | 41.16 |\n",
      "| ConvS2S Ensemble                               | 26.36 | 41.29 |\n",
      "| Transformer (base model)                      | 27.3  | 38.1  |\n",
      "| Transformer (big)                             | 28.4  | 41.8  |\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"Tell me BLUE scores for all model types in tabular format\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Variation | N | dmodel | dff  | h  | dk | dv | Pdrop | ϵls | train steps | PPL (dev) | BLEU (dev) | params × 10^6 |\n",
      "|-----------|---|--------|------|----|----|----|-------|-----|-------------|-----------|------------|----------------|\n",
      "| base      | 6 | 512    | 2048 | 8  | 64 | 64 | 0.1   | 0.1 | 100K        | 4.92      | 25.8       | 65             |\n",
      "| (A)       | 1 | 512    | 512  |    |    |    |       |     |             | 5.29      | 24.9       |                |\n",
      "|           | 4 | 128    | 128  |    |    |    |       |     |             | 5.00      | 25.5       |                |\n",
      "|           | 16| 32     | 32   |    |    |    |       |     |             | 4.91      | 25.8       |                |\n",
      "|           | 32| 16     | 16   |    |    |    |       |     |             | 5.01      | 25.4       |                |\n",
      "| (B)       |   |        |      |    |    |    |       |     |             | 5.16      | 25.1       | 58             |\n",
      "|           |   |        |      |    |    |    |       |     |             | 5.01      | 25.4       | 60             |\n",
      "|           | 2 |        |      |    |    |    |       |     |             | 6.11      | 23.7       | 36             |\n",
      "|           | 4 |        |      |    |    |    |       |     |             | 5.19      | 25.3       | 50             |\n",
      "|           | 8 |        |      |    |    |    |       |     |             | 4.88      | 25.5       | 80             |\n",
      "| (C)       |   | 256    |      |    | 32 | 32 |       |     |             | 5.75      | 24.5       | 28             |\n",
      "|           |   | 1024   |      | 128| 128|    |       |     |             | 4.66      | 26.0       | 168            |\n",
      "|           |   | 1024   |      |    |    |    |       |     |             | 5.12      | 25.4       | 53             |\n",
      "|           |   | 4096   |      |    |    |    |       |     |             | 4.75      | 26.2       | 90             |\n",
      "| (D)       |   |        |      |    |    |    | 0.0   |     |             | 5.77      | 24.6       |                |\n",
      "|           |   |        |      |    |    |    | 0.2   |     |             | 4.95      | 25.5       |                |\n",
      "|           |   |        |      |    |    |    | 0.0   |     |             | 4.67      | 25.3       |                |\n",
      "|           |   |        |      |    |    |    | 0.2   |     |             | 5.47      | 25.7       |                |\n",
      "| (E)       |   |        |      |    |    |    |       |     |             | 4.92      | 25.7       |                |\n",
      "| big       | 6 | 1024   | 4096 | 16 |    |    | 0.3   |     | 300K        | 4.33      | 26.4       | 213            |\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what are the Variations on the Transformer architecture? tell me in a tabulart fashion\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
