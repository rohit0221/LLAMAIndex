{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model\n",
    "Here we use gpt-4o and default OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 640116c7-3331-4712-b8c0-2e6afd504c04\n",
      "."
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "docs = LlamaParse(result_type=\"text\").load_data(\"./paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "def get_sub_docs(docs):\n",
    "    \"\"\"Split docs into pages, by separator.\"\"\"\n",
    "    sub_docs = []\n",
    "    for doc in docs:\n",
    "        doc_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for doc_chunk in doc_chunks:\n",
    "            sub_doc = Document(\n",
    "                text=doc_chunk,\n",
    "                metadata=deepcopy(doc.metadata),\n",
    "            )\n",
    "            sub_docs.append(sub_doc)\n",
    "\n",
    "    return sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will split into pages\n",
    "sub_docs = get_sub_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Graph Store\n",
    "Here we use Neo4j but you can also use our other integrations like Nebula (see an example notebook).\n",
    "\n",
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command\n",
    "\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "From here, you can open the db at http://localhost:7474/. On this page, you will be asked to sign in. Use the default username/password of neo4j and neo4j.\n",
    "\n",
    "Once you login for the first time, you will be asked to change the password.\n",
    "\n",
    "After this, you are ready to create your first property graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.environ.get('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.environ.get('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "graph_store = Neo4jPGStore(\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neo4j+s://d7ec37c2.databases.neo4j.io'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEO4J_USERNAME\n",
    "NEO4J_PASSWORD\n",
    "NEO4J_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 41/41 [00:00<00:00, 189.82it/s]\n",
      "Extracting implicit paths: 100%|██████████| 56/56 [00:00<00:00, 56151.33it/s]\n",
      "Extracting paths from text: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.38s/it]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:04<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "index = PropertyGraphIndex.from_documents(\n",
    "    sub_docs,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this if index is already loaded !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PropertyGraphIndex.from_existing(\n",
    "    graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vector Retriever\n",
    "Here we define our vector context retriever - it returns initial nodes via vector search, and traverses the relations to pull in more nodes/context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "\n",
    "kg_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    similarity_top_k=2,\n",
    "    path_depth=1,\n",
    "    # include_text=False,\n",
    "    include_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ">> IDX: 0, Here are some facts extracted from the provided text:\n",
      "\n",
      "Llama -> Is -> Open and efficient foundation language models\n",
      "\n",
      ": Generative multimodal models are in-context learners. arXiv\n",
      "       preprint arXiv:2312.13286 (2023)\n",
      "106. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R.,\n",
      "       Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable\n",
      "       multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n",
      "107. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,\n",
      "       H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for\n",
      "       dialog applications. arXiv preprint arXiv:2201.08239 (2022)\n",
      "108. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? explor-\n",
      "       ing the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209\n",
      "       (2024)\n",
      "109. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\n",
      "       Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\n",
      "       foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n",
      "110. Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multi-\n",
      "       modal few-shot learning with frozen language models. NeurIPS (2021)\n",
      "111. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image descrip-\n",
      "       tion evaluation. arXiv preprint arXiv:1411.5726 (2014)\n",
      "112. Wang, F., Mei, J., Yuille, A.: Sclip: Rethinking self-attention for dense vision-\n",
      "       language inference. arXiv preprint arXiv:2312.01597 (2023)\n",
      ">> IDX: 1, Here are some facts extracted from the provided text:\n",
      "\n",
      "Chung -> Is author of -> Scaling instruction-finetuned language models\n",
      "\n",
      "In: ICCV (2023)\n",
      "17. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz,\n",
      "      C.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual\n",
      "      vision and language model. arXiv preprint arXiv:2305.18565 (2023)\n",
      "18. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:\n",
      "      Microsoft coco captions: Data collection and evaluation server. arXiv preprint\n",
      "      arXiv:1504.00325 (2015)\n",
      "19. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\n",
      "      Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\n",
      "      guage modeling with pathways. JMLR (2023)\n",
      "20. Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang,\n",
      "      B., Wei, X., et al.: Mobilevlm: A fast, reproducible and strong vision language\n",
      "      assistant for mobile devices. arXiv preprint arXiv:2312.16886 (2023)\n",
      "21. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\n",
      "      Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models.\n",
      "      arXiv preprint arXiv:2210.11416 (2022)\n",
      "22. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord,\n",
      "      O.: Think you have solved question answering? try arc, the ai2 reasoning chal-\n",
      "      lenge. arXiv preprint arXiv:1803.05457 (2018)\n"
     ]
    }
   ],
   "source": [
    "nodes = kg_retriever.retrieve(\n",
    "    \"explain me Encoder and Decoder Stacks\"\n",
    ")\n",
    "# nodes = kg_retriever.retrieve('san francisco')\n",
    "print(len(nodes))\n",
    "for idx, node in enumerate(nodes):\n",
    "    print(f\">> IDX: {idx}, {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Baseline Vector Index\n",
    "We also build a \"baseline\" vector index. This follows the \"naive\" RAG pipeline approach of chunking and vector embedding. We use this as a comparison point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "base_index = VectorStoreIndex.from_documents(sub_docs, embed_model=embed_model)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=2)\n",
    "base_query_engine = RetrieverQueryEngine(base_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the AFM-Multimodal v3 model, detailing its capabilities in processing and extracting information from multimodal data, specifically images and text. It highlights the distinction between evaporation and evapotranspiration, using a flowchart to illustrate the processes involved in project suggestion and decision-making within a team context. The flowchart is color-coded to differentiate between actions and decisions, providing a clear visual representation of the sequence of steps. Additionally, the document outlines the dataset construction process, which includes interleaved image-text documents and text-only data, emphasizing the filtering and de-duplication methods used to ensure high-quality inputs. Overall, it showcases the model's ability to handle complex multimodal tasks and the structured approach taken in data preparation.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"Tell me over all summary of the document\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4-shot result numbers across all models of the MM1 ablation with different image encoders are as follows:\n",
      "\n",
      "- AIM600M: 56.6\n",
      "- AIM3B: 60.9\n",
      "- CLIPDFN+VeCap (ViT-L): 62.6\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "- CLIPDFN+VeCap (ViT-H): 60.0\n",
      "- CLIPOpenAI (ViT-L): 62.2\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "\n",
      "These values indicate the performance of each model under the 4-shot scenario.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"Tell me about 4-shot result numbers  across all models of MM1 ablation across different image encoders\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token size for the test-only data type in pre-training data ablation is not explicitly mentioned in the provided information.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what is the token size for test only data type  in pre training data ablation?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what are the Variations on the Transformer architecture? tell me in a tabulart fashion\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Custom Retriever\n",
    "Build joint retriever that combines vector and KG search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both KG vector search and direct vector search.\"\"\"\n",
    "\n",
    "    def __init__(self, kg_retriever, vector_retriever):\n",
    "        self._kg_retriever = kg_retriever\n",
    "        self._vector_retriever = vector_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        unique_nodes = {n.node_id: n for n in kg_nodes}\n",
    "        unique_nodes.update({n.node_id: n for n in vector_nodes})\n",
    "        return list(unique_nodes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_retriever = CustomRetriever(kg_retriever, base_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = custom_retriever.retrieve(\n",
    "    \"Give me all the programs that the mayor's budget includes\"\n",
    ")\n",
    "# len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent\n",
    "Now that we have the retriever, we can treat it as a RAG pipeline tool, and wrap it with an agent that can perform basic CoT reasoning and maintain conversation memory over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "kg_query_engine = RetrieverQueryEngine(custom_retriever)\n",
    "kg_query_tool = QueryEngineTool(\n",
    "    query_engine=kg_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"query_tool\",\n",
    "        description=\"Provides information about Methods, Analysis & Insights from Multimodal LLM Pre-training.\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [kg_query_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    ")\n",
    "agent = agent_worker.as_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about 4-shot result numbers  across all models of MM1 ablation across different image encoders\n",
      "=== Calling Function ===\n",
      "Calling function: query_tool with args: {\"input\": \"4-shot result numbers across all models of MM1 ablation across different image encoders\"}\n",
      "=== Function Output ===\n",
      "The 4-shot result numbers across all models of MM1 ablation across different image encoders are as follows:\n",
      "\n",
      "- AIM600M: 56.6\n",
      "- AIM1B: 59.5\n",
      "- AIM3B: 60.9\n",
      "- CLIPDFN+VeCap (ViT-L): 62.6\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "- CLIPOpenAI (ViT-L): 62.2\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "- CLIPDFN+VeCap (ViT-H): 63.6\n",
      "\n",
      "These values represent the performance of each model in the 4-shot evaluation setting.\n",
      "=== LLM Response ===\n",
      "The 4-shot result numbers across all models of MM1 ablation across different image encoders are as follows:\n",
      "\n",
      "- **AIM600M**: 56.6\n",
      "- **AIM1B**: 59.5\n",
      "- **AIM3B**: 60.9\n",
      "- **CLIPDFN+VeCap (ViT-L)**: 62.6\n",
      "- **CLIPDFN (ViT-H)**: 62.5\n",
      "- **CLIPOpenAI (ViT-L)**: 62.2\n",
      "- **CLIPDFN (ViT-H)**: 62.5\n",
      "- **CLIPDFN+VeCap (ViT-H)**: 63.6\n",
      "\n",
      "These values indicate the performance of each model in the 4-shot evaluation setting.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me about 4-shot result numbers  across all models of MM1 ablation across different image encoders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: what is the token size for test only data type  in pre training data ablation?\n",
      "=== Calling Function ===\n",
      "Calling function: query_tool with args: {\"input\": \"token size for test only data type in pre training data ablation\"}\n",
      "=== Function Output ===\n",
      "The token size for the text-only data type in the pre-training data ablation is 2 trillion tokens.\n",
      "=== LLM Response ===\n",
      "The token size for the text-only data type in the pre-training data ablation is 2 trillion tokens.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"what is the token size for test only data type  in pre training data ablation?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
