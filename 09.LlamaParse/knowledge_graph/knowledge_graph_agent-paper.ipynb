{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model\n",
    "Here we use gpt-4o and default OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3e7fcf17-d394-41dd-866f-414ccfcaa9cb\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "docs = LlamaParse(result_type=\"text\").load_data(\"./paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "def get_sub_docs(docs):\n",
    "    \"\"\"Split docs into pages, by separator.\"\"\"\n",
    "    sub_docs = []\n",
    "    for doc in docs:\n",
    "        doc_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for doc_chunk in doc_chunks:\n",
    "            sub_doc = Document(\n",
    "                text=doc_chunk,\n",
    "                metadata=deepcopy(doc.metadata),\n",
    "            )\n",
    "            sub_docs.append(sub_doc)\n",
    "\n",
    "    return sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will split into pages\n",
    "sub_docs = get_sub_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Graph Store\n",
    "Here we use Neo4j but you can also use our other integrations like Nebula (see an example notebook).\n",
    "\n",
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command\n",
    "\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "From here, you can open the db at http://localhost:7474/. On this page, you will be asked to sign in. Use the default username/password of neo4j and neo4j.\n",
    "\n",
    "Once you login for the first time, you will be asked to change the password.\n",
    "\n",
    "After this, you are ready to create your first property graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = os.environ.get('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.environ.get('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "graph_store = Neo4jPGStore(\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neo4j+s://d7ec37c2.databases.neo4j.io'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEO4J_USERNAME\n",
    "NEO4J_PASSWORD\n",
    "NEO4J_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 41/41 [00:00<00:00, 189.82it/s]\n",
      "Extracting implicit paths: 100%|██████████| 56/56 [00:00<00:00, 56151.33it/s]\n",
      "Extracting paths from text: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.38s/it]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:04<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "index = PropertyGraphIndex.from_documents(\n",
    "    sub_docs,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this if index is already loaded !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PropertyGraphIndex.from_existing(\n",
    "    graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vector Retriever\n",
    "Here we define our vector context retriever - it returns initial nodes via vector search, and traverses the relations to pull in more nodes/context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "\n",
    "kg_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    similarity_top_k=2,\n",
    "    path_depth=1,\n",
    "    # include_text=False,\n",
    "    include_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ">> IDX: 0, Here are some facts extracted from the provided text:\n",
      "\n",
      "Llama -> Is -> Open and efficient foundation language models\n",
      "\n",
      ": Generative multimodal models are in-context learners. arXiv\n",
      "       preprint arXiv:2312.13286 (2023)\n",
      "106. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R.,\n",
      "       Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable\n",
      "       multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n",
      "107. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,\n",
      "       H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for\n",
      "       dialog applications. arXiv preprint arXiv:2201.08239 (2022)\n",
      "108. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? explor-\n",
      "       ing the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209\n",
      "       (2024)\n",
      "109. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\n",
      "       Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\n",
      "       foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n",
      "110. Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multi-\n",
      "       modal few-shot learning with frozen language models. NeurIPS (2021)\n",
      "111. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image descrip-\n",
      "       tion evaluation. arXiv preprint arXiv:1411.5726 (2014)\n",
      "112. Wang, F., Mei, J., Yuille, A.: Sclip: Rethinking self-attention for dense vision-\n",
      "       language inference. arXiv preprint arXiv:2312.01597 (2023)\n",
      ">> IDX: 1, Here are some facts extracted from the provided text:\n",
      "\n",
      "Chung -> Is author of -> Scaling instruction-finetuned language models\n",
      "\n",
      "In: ICCV (2023)\n",
      "17. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz,\n",
      "      C.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual\n",
      "      vision and language model. arXiv preprint arXiv:2305.18565 (2023)\n",
      "18. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:\n",
      "      Microsoft coco captions: Data collection and evaluation server. arXiv preprint\n",
      "      arXiv:1504.00325 (2015)\n",
      "19. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\n",
      "      Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\n",
      "      guage modeling with pathways. JMLR (2023)\n",
      "20. Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang,\n",
      "      B., Wei, X., et al.: Mobilevlm: A fast, reproducible and strong vision language\n",
      "      assistant for mobile devices. arXiv preprint arXiv:2312.16886 (2023)\n",
      "21. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\n",
      "      Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models.\n",
      "      arXiv preprint arXiv:2210.11416 (2022)\n",
      "22. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord,\n",
      "      O.: Think you have solved question answering? try arc, the ai2 reasoning chal-\n",
      "      lenge. arXiv preprint arXiv:1803.05457 (2018)\n"
     ]
    }
   ],
   "source": [
    "nodes = kg_retriever.retrieve(\n",
    "    \"explain me Encoder and Decoder Stacks\"\n",
    ")\n",
    "# nodes = kg_retriever.retrieve('san francisco')\n",
    "print(len(nodes))\n",
    "for idx, node in enumerate(nodes):\n",
    "    print(f\">> IDX: {idx}, {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Baseline Vector Index\n",
    "We also build a \"baseline\" vector index. This follows the \"naive\" RAG pipeline approach of chunking and vector embedding. We use this as a comparison point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "base_index = VectorStoreIndex.from_documents(sub_docs, embed_model=embed_model)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=2)\n",
    "base_query_engine = RetrieverQueryEngine(base_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the AFM-Multimodal v3 model, detailing its capabilities in processing and extracting information from multimodal data, specifically focusing on image-text interactions. It highlights the distinction between evaporation and evapotranspiration, using a flowchart to illustrate the processes involved in project suggestion and decision-making within a team context. The document also describes the dataset construction process, which includes interleaved image-text documents and text-only data, emphasizing quality control measures such as image filtering and de-duplication. Overall, it presents a structured approach to understanding complex processes and the model's effectiveness in handling multimodal information.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"Tell me over all summary of the document\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4-shot result numbers across all models of the MM1 ablation with different image encoders are as follows:\n",
      "\n",
      "- AIM600M: 56.6\n",
      "- AIM3B: 60.9\n",
      "- CLIPDFN+VeCap (ViT-L): 62.6\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "- CLIPDFN+VeCap (ViT-H): 60.0\n",
      "- CLIPOpenAI (ViT-L): 62.2\n",
      "- CLIPDFN (ViT-H): 62.5\n",
      "\n",
      "These values reflect the performance of each model under the 4-shot scenario.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"Tell me about 4-shot result numbers  across all models of MM1 ablation across different image encoders\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token size for the test-only data type in pre-training data ablation is not explicitly mentioned in the provided information.\n"
     ]
    }
   ],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what is the token size for test only data type  in pre training data ablation?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = base_query_engine.query(\n",
    "    \"what are the Variations on the Transformer architecture? tell me in a tabulart fashion\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
