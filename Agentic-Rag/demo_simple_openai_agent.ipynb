{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-parse in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (0.5.12)\n",
      "Requirement already satisfied: llama-index in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (0.11.20)\n",
      "Collecting llama-index-postprocessor-sbert-rerank\n",
      "  Downloading llama_index_postprocessor_sbert_rerank-0.2.0-py3-none-any.whl.metadata (681 bytes)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-parse) (8.1.7)\n",
      "Requirement already satisfied: llama-index-core>=0.11.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-parse) (0.11.20)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.16)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.2.2)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: colorama in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from click<9.0.0,>=8.1.7->llama-parse) (0.4.6)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.52.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-core>=0.11.0->llama-parse) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.4)\n",
      "Requirement already satisfied: pandas in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: joblib in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core>=0.11.0->llama-parse) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core>=0.11.0->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core>=0.11.0->llama-parse) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama-parse) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core>=0.11.0->llama-parse) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from anyio->httpx->llama-index-core>=0.11.0->llama-parse) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama-parse) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\code\\github\\llamaindex\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (0.2.0)\n",
      "Downloading llama_index_postprocessor_sbert_rerank-0.2.0-py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: llama-index-postprocessor-sbert-rerank\n",
      "Successfully installed llama-index-postprocessor-sbert-rerank-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-parse llama-index llama-index-postprocessor-sbert-rerank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2403.09611.pdf -O paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing\n",
    "For parsing, lets use a recent [paper](https://huggingface.co/papers/2403.09611) on Multi-Modal pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2403.09611.pdf\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"paper.pdf\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can tell the parser to skip content we don't want. In this case, the references section will just add noise to a RAG system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 761b889d-60fd-41bc-91a5-34f908c25293\n",
      "..."
     ]
    }
   ],
   "source": [
    "documents = await parser.aload_data(\"paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, 2001.10it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, 2005.40it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 999.83it/s]\n",
      "1it [00:00, 1001.98it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "3it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "2it [00:00, 153.79it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "1it [00:00, 1020.02it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 1001.03it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.node_parser import (\n",
    "    MarkdownElementNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "\n",
    "# explicitly extract tables with the MarkdownElementNodeParser\n",
    "node_parser = MarkdownElementNodeParser(num_workers=8)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
    "\n",
    "# Chain splitters to ensure chunk size requirements are met\n",
    "nodes = SentenceSplitter(chunk_size=512, chunk_overlap=20).get_nodes_from_documents(\n",
    "    nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat over the paper, lets find out what it is about!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes)\n",
    "summary_index = SummaryIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Github\\LlamaIndex\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "# from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "tools = [\n",
    "    QueryEngineTool(\n",
    "        vector_index.as_query_engine(\n",
    "            similarity_top_k=8, node_postprocessors=[FlagEmbeddingReranker(top_n=3)]\n",
    "        ),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"search\",\n",
    "            description=\"Search the document, pass the entire user message in the query\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        summary_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"summarize\",\n",
    "            description=\"Summarize the document using the user message\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = OpenAIAgent.from_tools(tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the summary of the paper?\n",
      "=== Calling Function ===\n",
      "Calling function: summarize with args: {\"input\":\"What is the summary of the paper?\"}\n",
      "Got output: The paper discusses the development of Multimodal Large Language Models (MLLMs), focusing on the design and training processes that lead to high performance. It emphasizes the significance of various architectural components and data choices in building effective models. Key findings include the importance of a balanced mix of image-caption, interleaved image-text, and text-only data for achieving state-of-the-art few-shot results across multiple benchmarks. The study reveals that the image encoder's resolution and token count significantly impact performance, while the design of the vision-language connector is less critical. The authors present a family of models, MM1, with sizes ranging from 3B to 64B parameters, demonstrating competitive performance after supervised fine-tuning on established multimodal benchmarks. The paper aims to provide insights and principles for the community to build robust MLLMs, highlighting the empirical nature of the model-building process and the lessons learned from extensive ablations.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note -- this will take a while with local LLMs, its sending every node in the document to the LLM\n",
    "resp = agent.chat(\"What is the summary of the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses the development of Multimodal Large Language Models (MLLMs), focusing on their design and training processes for high performance. It highlights the importance of various architectural components and data choices, emphasizing a balanced mix of image-caption, interleaved image-text, and text-only data for achieving state-of-the-art few-shot results across multiple benchmarks. Key findings include the significant impact of the image encoder's resolution and token count on performance, while the design of the vision-language connector is less critical. The authors introduce a family of models, MM1, ranging from 3B to 64B parameters, which demonstrate competitive performance after supervised fine-tuning on established multimodal benchmarks. The paper aims to provide insights and principles for building robust MLLMs, emphasizing the empirical nature of the model-building process and lessons learned from extensive ablations.\n"
     ]
    }
   ],
   "source": [
    "print(str(resp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: How do the authors evaluate their work?\n",
      "=== Calling Function ===\n",
      "Calling function: search with args: {\"input\":\"How do the authors evaluate their work?\"}\n",
      "Got output: The authors evaluate their work through the design and implementation of multimodal evaluation infrastructure, which includes model evaluations and experimentation. They also utilize text-based evaluation methods alongside multimodal evaluation approaches to assess the performance of their models.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = agent.chat(\"How do the authors evaluate their work?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors evaluate their work by designing and implementing multimodal evaluation infrastructure, which encompasses model evaluations and experimentation. They employ both text-based evaluation methods and multimodal evaluation approaches to assess the performance of their models.\n"
     ]
    }
   ],
   "source": [
    "print(str(resp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
